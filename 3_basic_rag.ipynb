{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isnâ€™t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filename = \"../data/gen_ai_course.txt\"\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "\n",
    "# Answer\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a RetrievalQA chain\n",
    "# Hint: Use ChatOpenAI, create a prompt, and use StrOutputParser\n",
    "# Hint: The chain should be an LCEL chain https://python.langchain.com/v0.1/docs/expression_language/get_started/\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    max_tokens=None,\n",
    "    timeout=20,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", f\"You are a question-answering chatbot. You must provide the answer in {{language}}.\"),\n",
    "        (\"human\", f\"The question is: {{question}}\\n\\nRelevant Information:\\n{{formatted_docs}}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_docs = format_docs(docs)\n",
    "qa_chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question: str, language: str = \"English\") -> str:\n",
    "    result = qa_chain.invoke({\n",
    "        \"language\": language,\n",
    "        \"question\": question,\n",
    "        \"formatted_docs\": formatted_docs\n",
    "    }).content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to query 1: This document provides a comprehensive overview of Large Language Models (LLMs), focusing heavily on the **architecture and training of transformer-based models**.  It covers topics such as pre-training, fine-tuning techniques (including Supervised Fine Tuning and RLHF), tokenization, and evaluation methods.  Additionally, it discusses Retrieval Augmented Generation (RAG) and its variations, along with the use of tools and agents with LLMs.  While other topics like model optimization and a brief history of pre-transformer architectures are touched upon, the core subject remains the workings and training of transformer models within the broader context of generative AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"What is the main topic discussed in the document?\"\n",
    "result_1 = get_answer(query_1)\n",
    "print(f\"Answer to query 1: {result_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to query 2: The key points discussed include:\n",
      "\n",
      "**Large Language Models (LLMs):**\n",
      "\n",
      "* **Building LLMs:**  Involves pre-training (using cross-entropy loss, tokenization, data preprocessing, scaling laws) and post-training (fine-tuning with supervised fine-tuning, RLHF using reward models and PPO/DPO).  Cost and optimization are important considerations during training.\n",
      "* **Evaluation:**  Uses datasets like IFEval, BBH, MMLU-Pro, and Math, covering diverse fields.  Contamination of training data is a concern.\n",
      "* **Supervised Fine-Tuning (SFT):** Aligns LLMs to follow instructions and human preferences, addressing limitations of pre-training alone. Data collection for SFT can be scaled using LLMs.\n",
      "* **Reinforcement Learning from Human Feedback (RLHF):** Improves alignment further by training a reward model based on human preferences for different generated answers.  PPO and DPO are used for training the RL model.  RLHF faces challenges like answer length inflation and human preference inconsistencies.\n",
      "\n",
      "**Transformers:**\n",
      "\n",
      "* **History:**  N-grams, embeddings, RNNs, and LSTMs predate transformers.  Transformers address limitations of these earlier models, particularly vanishing/exploding gradients and limited context windows.\n",
      "* **Architecture:** Key components include self-attention/cross-attention, multi-head attention, residual connections, layer normalization, feed-forward layers, softmax layer, and positional embeddings.  Self-attention allows the model to weigh the importance of different parts of the input sequence.  Positional embeddings provide information about word order.\n",
      "\n",
      "**Retrieval Augmented Generation (RAG):**\n",
      "\n",
      "* **Basics:** Combines retrieval-based and generation-based models to access external knowledge.  This improves accuracy, handles recent events, and reduces hallucinations.\n",
      "* **Information Retrieval:** Methods include TF-IDF, BM25, cosine similarity, Euclidean distance, and Maximal Marginal Relevance (MMR).  Sparse and dense retrieval methods can be combined.\n",
      "* **Vectorstores and Search Optimization:** Vector databases store embeddings.  Efficient similarity search methods include ScaNN, FAISS, and HNSW.  Rerankers improve retrieval accuracy.\n",
      "* **RAG Techniques:** Query augmentation, query rephrasing, prompt engineering, document loaders, chunking, and context management.\n",
      "* **Evaluation:** Precision, recall, NDCG, and LLM-based evaluation.\n",
      "* **Multimodal RAG:** Incorporates multiple data types like text and images.\n",
      "* **SOTA Architectures:** Self-RAG, RAPTOR, Corrective RAG (CRAG), and GraphRAG.\n",
      "\n",
      "**Tools and Agents:**\n",
      "\n",
      "* Tools allow LLMs to interact with external systems and data.  Agents use tools to perform complex tasks.  (This section is briefly mentioned but lacks detail in the provided text.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"Can you summarize the key points mentioned?\"\n",
    "result_2 = get_answer(query_2)\n",
    "print(f\"Answer to query 2: {result_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! Youâ€™ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
