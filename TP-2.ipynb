{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyvEF4bInDV9"
   },
   "source": [
    "### What is Attention?\n",
    "\n",
    "Attention mechanisms allow models to focus on relevant parts of the input when generating each part of the output. In sequence-to-sequence tasks like machine translation, attention helps the model decide which words in the input sentence are most relevant to generating the next word in the output sentence.\n",
    "\n",
    "### Why is Attention Important?\n",
    "\n",
    "- **Handles Long Dependencies**: Captures relationships between distant elements in sequences.\n",
    "- **Improves Performance**: Enhances accuracy in tasks like translation, summarization, and question answering.\n",
    "- **Interpretability**: Provides insights into what the model is focusing on, making it more interpretable.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Query (Q)**: The vector representing the current focus point.\n",
    "- **Keys (K)**: The vectors representing all possible focus points.\n",
    "- **Values (V)**: The vectors containing the information to be aggregated.\n",
    "\n",
    "### Attention Calculation:\n",
    "\n",
    "1. **Score Calculation**: Compute a score between the query and each key.\n",
    "   - Example: Dot product between Q and K.\n",
    "2. **Weighting**: Apply a softmax function to obtain weights.\n",
    "   - Ensures that the weights sum to 1.\n",
    "3. **Aggregation**: Multiply the weights with the values and sum them up.\n",
    "\n",
    "**Attention:**\n",
    "\n",
    "<img src=\"../docs/attention.png\" alt=\"Alt Text\" width=\"500\"/>\n",
    "\n",
    "\n",
    "**Softmax:**\n",
    "\n",
    "<img src=\"../docs/softmax.png\" alt=\"Alt Text\" width=\"500\"/>\n",
    "\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the scaled dot-product attention mechanism from scratch.\n",
    "- Understand each step of the computation.\n",
    "- Verify the implementation with test cases.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1430,
     "status": "ok",
     "timestamp": 1730664497712,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "p_ZMNSWk-9qQ",
    "outputId": "f9c461bb-3411-48ef-c5bf-e29cc3a22ec3"
   },
   "outputs": [],
   "source": [
    "# Install the repository\n",
    "#!git clone https://github.com/BastinFlorian/GenAI-Dauphine-Course.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36635,
     "status": "ok",
     "timestamp": 1730664536903,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "PIP6bGCpBeCD",
    "outputId": "704b3b9c-8303-41a4-d830-b344771c01ec"
   },
   "outputs": [],
   "source": [
    "# Install the requirements\n",
    "#!pip install -r /content/GenAI-Dauphine-Course/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6500,
     "status": "ok",
     "timestamp": 1730664543396,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "pzlj8FNznDWB"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1730664779372,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "UWPdsTAHnDWE"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Compute the scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        q: Queries tensor of shape (batch_size, seq_length_q, d_k)\n",
    "        k: Keys tensor of shape (batch_size, seq_length_k, d_k)\n",
    "        v: Values tensor of shape (batch_size, seq_length_v, d_v)\n",
    "        mask: Optional mask tensor to prevent attention to certain positions.\n",
    "\n",
    "    Returns:\n",
    "        context: The result of the attention mechanism.\n",
    "        attention_weights: The weights assigned to each value.\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "\n",
    "    # TODO: Compute the dot products between queries and keys\n",
    "\n",
    "    # Compute the dot products between queries and keys, then scale\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # TODO: Apply softmax to get the attention weights\n",
    "\n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Compute the context vector as the weighted sum of values\n",
    "    context = torch.matmul(attention_weights, v)\n",
    "\n",
    "    return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1730664782578,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "R8ARbdnRnDWG",
    "outputId": "897844f5-a1cd-4dfb-be9f-04b0b0768050"
   },
   "outputs": [],
   "source": [
    "# Test the scaled_dot_product_attention function\n",
    "batch_size = 2\n",
    "seq_length_q = 5\n",
    "seq_length_k = 6\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "# Random tensors for queries, keys, and values\n",
    "q = torch.rand(batch_size, seq_length_q, d_k)\n",
    "k = torch.rand(batch_size, seq_length_k, d_k)\n",
    "v = torch.rand(batch_size, seq_length_k, d_v)\n",
    "\n",
    "# Optional mask (None for now)\n",
    "mask = None\n",
    "\n",
    "# Compute attention\n",
    "context, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "print(f\"Context shape: {context.shape}\")  # Expected: (batch_size, seq_length_q, d_v)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")  # Expected: (batch_size, seq_length_q, seq_length_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9BXtjX9nDWI"
   },
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "Multi-head attention allows the model to focus on different positions and represent the subspaces differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1730665370498,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "JsOgqbLCsQEL"
   },
   "outputs": [],
   "source": [
    "# Manque la pr√©sence de la fonction def split_heads(self, x, batch_size): dans class MultiHeadAttention(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1730665284777,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "s4Np8s1WnDWJ"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.q_linear(q)  # (batch_size, seq_length, d_model)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # TODO: Split into num_heads\n",
    "        # Split into num_heads\n",
    "        q = self.split_heads(q, batch_size)  # Shape: (batch_size, num_heads, seq_length, d_k)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        context, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.out_linear(context)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1730665289497,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "pReE9wENnDWK",
    "outputId": "60521b7f-838e-4e7b-801e-7d5d1dbc9c5c"
   },
   "outputs": [],
   "source": [
    "# Test the MultiHeadAttention class\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "\n",
    "# Random input tensor\n",
    "x = torch.rand(batch_size, seq_length, d_model)\n",
    "\n",
    "# Instantiate the MultiHeadAttention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (batch_size, seq_length, d_model)\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # Expected: (batch_size, num_heads, seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Nu4z0EmnDWM"
   },
   "source": [
    "# Visualizing Attention Weights\n",
    "\n",
    "Visualization helps in understanding which parts of the input the model is focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3717,
     "status": "ok",
     "timestamp": 1730665472918,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "mwfhhU5lnDWN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_attention_weights(attention_weights, layer=0, head=0):\n",
    "    \"\"\"\n",
    "    Plots the attention weights.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: Tensor of attention weights of shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        layer: The layer number (if applicable)\n",
    "        head: The head number to visualize\n",
    "    \"\"\"\n",
    "    # Select the first batch\n",
    "    attn = attention_weights[0, head].detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn, cmap='viridis')\n",
    "    plt.title(f'Attention Weights - Layer {layer+1}, Head {head+1}')\n",
    "    plt.xlabel('Key Positions')\n",
    "    plt.ylabel('Query Positions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1730665524289,
     "user": {
      "displayName": "Samar Krimi",
      "userId": "13190416286904657726"
     },
     "user_tz": -60
    },
    "id": "1UMbgljInDWP",
    "outputId": "87ea7b73-f2cc-4d6d-f06e-00cb0b424d48"
   },
   "outputs": [],
   "source": [
    "# Assume we have attention weights from the previous example\n",
    "plot_attention_weights(attn_weights, layer=0, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCNL1lnCuq8T"
   },
   "source": [
    "* Sparse Attention Pattern :\n",
    "The sparsity of bright cells across the heatmap suggests that the attention mechanism is selective, focusing on certain positions rather than uniformly distributing attention. This can be beneficial as it allows the model to prioritize relevant parts of the input sequence."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
