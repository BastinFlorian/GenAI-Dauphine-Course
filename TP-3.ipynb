{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wil1STmDlEVy"
   },
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file.\n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5K09LMalEV3"
   },
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "IyceGQBilEV4"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "from langchain import LLMChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=r'C:\\Users\\USER\\Desktop\\GENERATIVE AI DAUPHINE\\GenAI-Dauphine-Course\\env.template')\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables d'environnement chargées avec succès.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "success = load_dotenv(dotenv_path='.env.template')\n",
    "\n",
    "if success:\n",
    "    print(\"Variables d'environnement chargées avec succès.\")\n",
    "else:\n",
    "    print(\"Erreur lors du chargement des variables d'environnement.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ShjtaELn-xQ",
    "outputId": "f912c917-791f-4f3f-b2e3-a31ec2b28ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clé API Google chargée avec succès.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv(dotenv_path='.env.template')\n",
    "\n",
    "# Vérifiez si les variables sont chargées correctement\n",
    "import os\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if google_key:\n",
    "    print(\"Clé API Google chargée avec succès.\")\n",
    "else:\n",
    "    print(\"Erreur: Clé API Google non chargée.\")\n",
    "    # Afficher toutes les variables d'environnement pour déboguer\n",
    "    print(\"Variables d'environnement actuelles:\", os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mOlyYVslEV7"
   },
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "VpsLbSYau0Uv"
   },
   "outputs": [],
   "source": [
    "# Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "# Specify the filename\n",
    "filename = r\"C:\\Users\\USER\\Desktop\\GENERATIVE AI DAUPHINE\\GenAI-Dauphine-Course\\data\\gen_ai_course.txt\"\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(filename, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 243\n"
     ]
    }
   ],
   "source": [
    "# Output the number of chunks created for verification\n",
    "print(f\"Number of chunks created: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc8iZYjRlEV9"
   },
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and store them in a VectorStore\n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "# Create a FAISS vector store for efficient similarity search\n",
    "vector_store = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 768\n"
     ]
    }
   ],
   "source": [
    "# Tester avec un texte\n",
    "text = \"La nature est un trésor précieux à préserver.\"\n",
    "embedding_result = embeddings.embed_query(text)\n",
    "\n",
    "# Afficher l'embedding\n",
    "# Limiter l'affichage aux 10 premiers éléments\n",
    "# Afficher un résumé de l'embedding, par exemple, en affichant la forme ou la taille du vecteur\n",
    "print(f\"Embedding size: {len(embedding_result)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFKHRU-hlEV-"
   },
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL\n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "Préserver la nature est crucial, non seulement pour notre propre survie, mais aussi pour le bien-être de la planète entière.  La nature est un système interconnecté et complexe dont nous dépendons entièrement.  Voici quelques raisons pour lesquelles sa préservation est primordiale :\n",
      "\n",
      "* **Notre survie dépend de la nature:**  L'air que nous respirons, l'eau que nous buvons et la nourriture que nous mangeons proviennent tous de la nature.  Les écosystèmes naturels purifient l'air et l'eau, régulent le climat et pollinisent nos cultures. Détruire la nature, c'est scier la branche sur laquelle nous sommes assis.\n",
      "\n",
      "* **La biodiversité est essentielle:** La nature abrite une incroyable diversité d'espèces animales et végétales.  Chaque espèce joue un rôle important dans l'équilibre des écosystèmes.  La perte de biodiversité fragilise ces écosystèmes, les rendant plus vulnérables aux maladies et aux changements climatiques.  Imaginez un monde sans le chant des oiseaux, le bourdonnement des abeilles ou la beauté des fleurs sauvages.\n",
      "\n",
      "* **La nature a une valeur intrinsèque:**  Au-delà de son utilité pour l'homme, la nature a une valeur en soi.  Chaque être vivant, chaque écosystème a le droit d'exister.  Nous avons la responsabilité morale de protéger la beauté et la richesse de la nature\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle Google Generative AI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",  # Modèle disponible\n",
    "    temperature=0.7,        # Contrôle la créativité\n",
    "    max_output_tokens=300,  # Limite de la longueur de la réponse\n",
    "    timeout=15,             # Délai maximum pour attendre une réponse\n",
    "    max_retries=3           # Nombre de tentatives en cas d'échec\n",
    ")\n",
    "\n",
    "# Création d'un prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that emphasizes the importance of nature and preserving the environment.\"),\n",
    "    (\"human\", \"{query}\")  # Placeholder pour intégrer des requêtes dynamiques\n",
    "])\n",
    "\n",
    "#Create a function to format documents for the prompt\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Configuration de la chaîne QA en combinant le prompt et le model\n",
    "qa_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Fournir une question à la chaîne QA\n",
    "query = \"Pourquoi est-il important de préserver la nature ?\"\n",
    "response = qa_chain.run({\"query\": query})\n",
    "\n",
    "# Affichage de la réponse\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdJnP9B_lEWA"
   },
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The document emphasizes the urgent need for environmental conservation by protecting our precious natural ecosystems and the rich biodiversity they harbor.  It underscores the detrimental impact of human activities and explores strategies to lessen our footprint and foster a healthier relationship with the natural world.  Essentially, the core message is a call to action to safeguard our planet's future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple de document à utiliser avec le modèle\n",
    "docs = [\n",
    "    Document(page_content=\"This document discusses the urgent need to protect natural ecosystems and biodiversity. It highlights the impact of human activities on the environment and proposes strategies to mitigate these effects.\"),\n",
    "]\n",
    "\n",
    "# Fonction pour formater les documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Formater les documents\n",
    "formatted_docs = format_docs(docs)\n",
    "\n",
    "# Définir une question précise et directe\n",
    "query = f\"Can you summarize the main topic discussed in the following document, focusing on environmental conservation?\\n\\n{formatted_docs}\"\n",
    "\n",
    "# Interroger la chaîne QA avec plus de contexte\n",
    "result = qa_chain.invoke({\n",
    "    \"query\": query,\n",
    "})\n",
    "\n",
    "# Afficher uniquement la réponse du modèle\n",
    "\n",
    "print(f\"Answer: {result['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85yFfWxnlEWD"
   },
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming, driven by human activities releasing greenhouse gases into the atmosphere, is significantly impacting our precious planet, affecting both natural systems and human society.  It's crucial to remember that the Earth's delicate ecosystems are interconnected, and changes in one area can have ripple effects across the globe.\n",
      "\n",
      "Here are some key impacts:\n",
      "\n",
      "**On Natural Systems:**\n",
      "\n",
      "* **Rising Temperatures:**  The most direct impact is the increase in average global temperatures, both on land and in the oceans. This warming disrupts natural cycles and stresses ecosystems.  Think of the delicate balance of a forest, dependent on specific temperature ranges for its flora and fauna to thrive.\n",
      "* **Melting Ice and Rising Sea Levels:**  As temperatures rise, glaciers and polar ice caps melt at an alarming rate, contributing to rising sea levels. This threatens coastal communities and vital habitats like coral reefs and mangrove forests, which are crucial for biodiversity.\n",
      "* **Extreme Weather Events:** Global warming intensifies weather patterns, leading to more frequent and severe heatwaves, droughts, floods, wildfires, and storms. These events devastate natural landscapes, displace wildlife, and endanger human lives. Imagine the impact of a wildfire on a forest ecosystem, destroying habitats and releasing even more carbon into the atmosphere.\n",
      "* **Ocean Acidification:** The ocean absorbs a significant portion of the excess carbon dioxide in the atmosphere, leading to increased acidity. This harms marine life, particularly shellfish and corals, which rely on specific pH levels for shell formation.  \n"
     ]
    }
   ],
   "source": [
    "# Remplacer 'Another question here' par votre question et exécuter la chaîne QA pour cette question\n",
    "\n",
    "query = \"What is the impact of global warming?\"\n",
    "result = qa_chain.invoke(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Afficher uniquement la réponse du modèle\n",
    "print(result['text'])  # Affiche la réponse générée\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfJH8VZalEWD"
   },
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Utilisation d'un autre modèle de langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de changement de modèle\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gpt-4\",  # Remplacer par un autre modèle\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=300,\n",
    "    timeout=15,\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxNXVoT5lEWE"
   },
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSVDf4EBlEWE"
   },
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Y8DMsNwlEWE",
    "outputId": "47d3fc95-82dc-4c0a-891f-3c5aa228723c"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
