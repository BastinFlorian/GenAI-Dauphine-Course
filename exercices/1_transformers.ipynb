{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer Model from Scratch with PyTorch and HuggingFace\n",
    "\n",
    "In this notebook, we'll walk through the process of building a Transformer model from scratch using PyTorch and HuggingFace. Transformers have revolutionized the field of natural language processing by enabling models to understand context and relationships in sequential data more effectively.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Implement each component of the Transformer architecture step by step.\n",
    "- Fill in the `TODO` sections to complete the implementation.\n",
    "- Use test cells provided after each section to verify your implementation.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture Overview\n",
    "\n",
    "A Transformer model primarily consists of an Encoder and a Decoder. Each of these is made up of multiple layers that include mechanisms like self-attention and feed-forward neural networks.\n",
    "\n",
    "In this notebook, we'll focus on implementing the **Encoder** part of the Transformer, which is commonly used for tasks like text classification or encoding sequences for downstream tasks.\n",
    "\n",
    "The encoder layer is like:\n",
    "\n",
    "<img src=\"../docs/encoder_layer.png\" alt=\"Alt Text\" width=\"500\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing the Transformer Encoder Layer\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `TransformerEncoderLayer` class.\n",
    "- You'll define the feedforward neural network components.\n",
    "- You'll complete the `forward` method to process the input through the self-attention and feedforward layers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "       ###### Define the feedforward neural network layers ######\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)  # First linear layer\n",
    "        self.dropout = nn.Dropout(dropout)                  # Dropout layer\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)  # Second linear layer\n",
    "\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Self-attention layer\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "\n",
    "        ####### Implement the feedforward neural network part ######\n",
    "        src2 = self.linear1(src)          # First linear transformation\n",
    "        src2 = torch.relu(src2)           # ReLU activation function\n",
    "        src2 = self.dropout(src2)         # Apply dropout\n",
    "        src = src + self.dropout2(self.linear2(src2))  # Residual connection and second linear transformation\n",
    "        src = self.norm2(src)             # Apply layer normalization\n",
    "\n",
    "        return src\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Transformer Encoder Layer\n",
    "\n",
    "Let's create an instance of `TransformerEncoderLayer` and pass some dummy data through it to ensure it's working correctly.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Create dummy input data.\n",
    "- Initialize the `TransformerEncoderLayer`.\n",
    "- Pass the data through the layer and observe the output shape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test the TransformerEncoderLayer\n",
    "# Parameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# Create an instance of the TransformerEncoderLayer\n",
    "encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "\n",
    "# Dummy input data (sequence length, batch size, embedding size)\n",
    "seq_length = 10\n",
    "batch_size = 2\n",
    "dummy_input = torch.rand(seq_length, batch_size, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder_layer(dummy_input)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should be `(seq_length, batch_size, d_model)`, which confirms that the encoder layer processes the input correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Full Transformer Encoder\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `TransformerEncoder` class.\n",
    "- You'll stack multiple `TransformerEncoderLayer` instances.\n",
    "- You'll complete the `forward` method to process the input through all the layers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask,\n",
    "                         src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Transformer Encoder\n",
    "\n",
    "Let's create an instance of `TransformerEncoder` and pass some dummy data through it.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Use the previously defined `TransformerEncoderLayer` as the building block.\n",
    "- Initialize the `TransformerEncoder` with multiple layers.\n",
    "- Pass the dummy data through the encoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after TransformerEncoder: torch.Size([10, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test the TransformerEncoder\n",
    "num_layers = 6\n",
    "\n",
    "# Initialize the TransformerEncoder\n",
    "transformer_encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "# Dummy input data remains the same\n",
    "# Forward pass\n",
    "output = transformer_encoder(dummy_input)\n",
    "\n",
    "print(f\"Output shape after TransformerEncoder: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should still be `(seq_length, batch_size, d_model)`, confirming that stacking multiple layers doesn't change the shape but refines the representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Positional Encoding\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `PositionalEncoding` class.\n",
    "- You'll initialize the positional encoding matrix using sine and cosine functions.\n",
    "- You'll complete the `forward` method to add positional encodings to the input embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"../docs/positional_encoding.png\" alt=\"Alt Text\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ###### TODO: Initialize the positional encoding matrix #####\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) \n",
    "\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Positional Encoding\n",
    "\n",
    "Let's verify that our positional encoding works by passing some dummy data through it.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Create dummy input embeddings.\n",
    "- Initialize the `PositionalEncoding` class.\n",
    "- Pass the embeddings through the positional encoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Embeddings shape: torch.Size([10, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test the PositionalEncoding\n",
    "pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "# Dummy input embeddings (sequence length, batch size, embedding size)\n",
    "dummy_embeddings = torch.zeros(seq_length, batch_size, d_model)\n",
    "\n",
    "# Forward pass\n",
    "pos_encoded_embeddings = pos_encoder(dummy_embeddings)\n",
    "\n",
    "print(f\"Positional Encoded Embeddings shape: {pos_encoded_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should be `(seq_length, batch_size, d_model)`, and the embeddings should now contain positional information.\n",
    "\n",
    "---\n",
    "\n",
    "## Assembling the Complete Transformer Model\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `TransformerModel` class.\n",
    "- You'll combine the embedding layer, positional encoding, transformer encoder, and the final decoder layer.\n",
    "- You'll initialize the weights of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        ##### TODO: Initialize the weights of the model #####\n",
    "        # Hint: Initialize the embedding and decoder weights uniformly, and set decoder biases to zero.\n",
    "        #####\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Complete Transformer Model\n",
    "\n",
    "Let's test the entire model by passing some dummy data through it.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Define the model parameters.\n",
    "- Create an instance of `TransformerModel`.\n",
    "- Generate dummy input data.\n",
    "- Perform a forward pass and observe the output shape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([35, 32, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "ntokens = 1000  # Size of vocabulary\n",
    "d_model = 512   # Embedding size\n",
    "nhead = 8       # Number of attention heads\n",
    "dim_feedforward = 2048  # Feedforward network hidden layer size\n",
    "num_layers = 6  # Number of encoder layers\n",
    "dropout = 0.2   # Dropout rate\n",
    "\n",
    "model = TransformerModel(ntokens, d_model, nhead, dim_feedforward, num_layers, dropout)\n",
    "\n",
    "# Dummy input data (sequence length, batch size)\n",
    "batch_size = 32\n",
    "seq_length = 35\n",
    "input_data = torch.randint(0, ntokens, (seq_length, batch_size))\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [seq_length, batch_size, ntokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should be `(seq_length, batch_size, ntokens)`, indicating that the model outputs a score for each token in the vocabulary at each position in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We've successfully built a Transformer Encoder model from scratch using PyTorch. This implementation covers the key components of the Transformer architecture, including multi-head attention, positional encoding, and feedforward neural networks.\n",
    "\n",
    "Understanding how each part works and how they fit together is crucial for leveraging Transformers in your own projects.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Extend the model to include the Decoder part of the Transformer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the Transformer Model with a Decoder\n",
    "\n",
    "In the previous sections, we implemented the **Encoder** part of the Transformer model. Now, we'll extend our implementation to include the **Decoder** part, completing the Transformer architecture.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Implement the `TransformerDecoderLayer` and `TransformerDecoder` classes.\n",
    "- Integrate the decoder into the `TransformerModel`.\n",
    "- Test each component with provided test cells.\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Transformer Decoder\n",
    "\n",
    "The **Decoder** is responsible for generating the output sequence, one element at a time, while attending to the encoder's output. It consists of:\n",
    "\n",
    "- **Self-Attention Layer**: Allows the decoder to attend to previous positions in the output sequence.\n",
    "- **Cross-Attention Layer**: Allows the decoder to attend to the encoder's output.\n",
    "- **Feedforward Neural Network**: Processes the attention outputs.\n",
    "- **Layer Normalization and Residual Connections**: Applied after each sub-layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing the Transformer Decoder Layer\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `TransformerDecoderLayer` class.\n",
    "- You'll define the self-attention, cross-attention, and feedforward components.\n",
    "- You'll complete the `forward` method to process the input.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Decoder Layer\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # TODO: Define the feedforward neural network layers\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # Layer normalization and dropout layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Self-attention layer\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # TODO: Implement the cross-attention layer\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                    key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)  \n",
    "        tgt = self.norm2(tgt)  \n",
    "\n",
    "\n",
    "        # Feedforward layer\n",
    "        tgt2 = self.linear2(self.dropout(torch.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the Transformer Model with a Decoder\n",
    "\n",
    "In the previous sections, we implemented the **Encoder** part of the Transformer model. Now, we'll extend our implementation to include the **Decoder** part, completing the Transformer architecture.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Implement the `TransformerDecoderLayer` and `TransformerDecoder` classes.\n",
    "- Integrate the decoder into the `TransformerModel`.\n",
    "- Test each component with provided test cells.\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Transformer Decoder\n",
    "\n",
    "The **Decoder** is responsible for generating the output sequence, one element at a time, while attending to the encoder's output. It consists of:\n",
    "\n",
    "- **Self-Attention Layer**: Allows the decoder to attend to previous positions in the output sequence.\n",
    "- **Cross-Attention Layer**: Allows the decoder to attend to the encoder's output.\n",
    "- **Feedforward Neural Network**: Processes the attention outputs.\n",
    "- **Layer Normalization and Residual Connections**: Applied after each sub-layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing the Transformer Decoder Layer\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `TransformerDecoderLayer` class.\n",
    "- You'll define the self-attention, cross-attention, and feedforward components.\n",
    "- You'll complete the `forward` method to process the input.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Decoder Layer\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # TODO: Define the feedforward neural network layers\n",
    "        # Hint: Similar to the encoder layer, use two linear layers with a ReLU activation and dropout in between.\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)  \n",
    "        self.dropout = nn.Dropout(dropout)                  \n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)  \n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Self-attention layer\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # TODO: Implement the cross-attention layer\n",
    "        # Hint: Use multihead attention where the query is the decoder input and the key and value are the encoder output.\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                    key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)  \n",
    "        tgt = self.norm2(tgt) \n",
    "\n",
    "        # Feedforward layer\n",
    "        tgt2 = self.linear2(self.dropout(torch.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Transformer Decoder Layer\n",
    "\n",
    "Let's create an instance of `TransformerDecoderLayer` and pass some dummy data through it.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Create dummy input data for the decoder and encoder outputs.\n",
    "- Initialize the `TransformerDecoderLayer`.\n",
    "- Pass the data through the layer and observe the output shape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([12, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test the TransformerDecoderLayer\n",
    "# Parameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# Create an instance of the TransformerDecoderLayer\n",
    "decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "\n",
    "# Dummy input data for the decoder (target sequence length, batch size, embedding size)\n",
    "tgt_seq_length = 12\n",
    "batch_size = 2\n",
    "dummy_tgt = torch.rand(tgt_seq_length, batch_size, d_model)\n",
    "seq_length = 10\n",
    "# Dummy memory from the encoder (source sequence length, batch size, embedding size)\n",
    "memory = torch.rand(seq_length, batch_size, d_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output = decoder_layer(dummy_tgt, memory,\n",
    "                       tgt_mask=None,\n",
    "                       memory_mask=None,\n",
    "                       tgt_key_padding_mask=None,\n",
    "                       memory_key_padding_mask=None)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should be `(tgt_seq_length, batch_size, d_model)`, confirming that the decoder layer processes the input correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## Building the Full Transformer Decoder\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the `TransformerDecoder` class.\n",
    "- You'll stack multiple `TransformerDecoderLayer` instances.\n",
    "- You'll complete the `forward` method to process the input through all the layers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Transformer Decoder\n",
    "\n",
    "Let's create an instance of `TransformerDecoder` and pass some dummy data through it.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Use the previously defined `TransformerDecoderLayer` as the building block.\n",
    "- Initialize the `TransformerDecoder` with multiple layers.\n",
    "- Pass the dummy data through the decoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after TransformerDecoder: torch.Size([12, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test the TransformerDecoder\n",
    "num_layers = 6\n",
    "\n",
    "# Initialize the TransformerDecoder\n",
    "transformer_decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "# Dummy input data remains the same\n",
    "# Forward pass\n",
    "output = transformer_decoder(dummy_tgt, memory)\n",
    "\n",
    "print(f\"Output shape after TransformerDecoder: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should still be `(tgt_seq_length, batch_size, d_model)`, confirming that stacking multiple layers doesn't change the shape but refines the representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Updating the Transformer Model with Decoder\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll update the `TransformerModel` class to include the decoder.\n",
    "- You'll adjust the `forward` method to handle both source and target inputs.\n",
    "- You'll ensure that masks are properly handled.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Model with Encoder and Decoder\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_ntoken, tgt_ntoken, d_model, nhead, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.tgt_mask = None\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        decoder_layers = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers)\n",
    "\n",
    "        self.src_encoder = nn.Embedding(src_ntoken, d_model)\n",
    "        self.tgt_encoder = nn.Embedding(tgt_ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, tgt_ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of the model\n",
    "        initrange = 0.1\n",
    "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"Generate a square mask for the sequence. Mask out future positions.\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        src_emb = self.src_encoder(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        memory = self.transformer_encoder(src_emb, src_mask)\n",
    "\n",
    "        tgt_emb = self.tgt_encoder(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_decoder(tgt_emb)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Masks\n",
    "\n",
    "In sequence-to-sequence models, masks are crucial for:\n",
    "\n",
    "- **Source Mask (`src_mask`)**: To handle padding in the source sequences.\n",
    "- **Target Mask (`tgt_mask`)**: To prevent the model from peeking ahead in the target sequence during training (causal masking).\n",
    "- **Memory Mask (`memory_mask`)**: To handle padding in the encoder outputs when used by the decoder.\n",
    "\n",
    "---\n",
    "\n",
    "### Testing the Complete Transformer Model with Decoder\n",
    "\n",
    "Let's test the updated model by passing some dummy data through it.\n",
    "\n",
    "**What you'll do:**\n",
    "\n",
    "- Define separate vocab sizes for source and target languages.\n",
    "- Create an instance of the updated `TransformerModel`.\n",
    "- Generate dummy input data for both source and target.\n",
    "- Create appropriate masks.\n",
    "- Perform a forward pass and observe the output shape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([30, 32, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "src_ntokens = 1000  # Size of source vocabulary\n",
    "tgt_ntokens = 1000  # Size of target vocabulary\n",
    "d_model = 512       # Embedding size\n",
    "nhead = 8           # Number of attention heads\n",
    "dim_feedforward = 2048  # Feedforward network hidden layer size\n",
    "num_layers = 6      # Number of encoder and decoder layers\n",
    "dropout = 0.2       # Dropout rate\n",
    "\n",
    "model = TransformerModel(src_ntokens, tgt_ntokens, d_model, nhead, dim_feedforward, num_layers, dropout)\n",
    "\n",
    "# Dummy input data\n",
    "batch_size = 32\n",
    "src_seq_length = 35\n",
    "tgt_seq_length = 30\n",
    "src_input = torch.randint(0, src_ntokens, (src_seq_length, batch_size))\n",
    "tgt_input = torch.randint(0, tgt_ntokens, (tgt_seq_length, batch_size))\n",
    "\n",
    "# Generate masks\n",
    "# TODO: Generate the target mask to prevent the decoder from attending to future positions\n",
    "# Hint: Use the generate_square_subsequent_mask method provided in the model\n",
    "tgt_mask = model.generate_square_subsequent_mask(tgt_seq_length)\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_input, tgt_input, tgt_mask=tgt_mask)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [tgt_seq_length, batch_size, tgt_ntokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "The output shape should be `(tgt_seq_length, batch_size, tgt_ntokens)`, indicating that the model outputs a score for each token in the target vocabulary at each position in the target sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We've successfully extended our Transformer model to include the **Decoder** part, completing the architecture. This comprehensive implementation covers:\n",
    "\n",
    "- Multi-head self-attention and cross-attention mechanisms.\n",
    "- Positional encoding for both encoder and decoder inputs.\n",
    "- Handling of masks to manage sequence padding and causal relationships.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Training the Model**: Use a dataset to train the model for tasks like machine translation or text summarization.\n",
    "- **Implementing Beam Search**: Enhance the decoding process by implementing beam search to generate more coherent sequences.\n",
    "- **Exploring Pre-trained Models**: Utilize HuggingFace's Transformers library to leverage pre-trained models for your tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.zeros`: Creates a tensor filled with zeros.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    ```\n",
    "- `torch.arange`: Returns a 1-D tensor of size `(end - start) / step` with values from `start` to `end` (exclusive) with step size `step`.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    ```\n",
    "- `torch.exp`: Returns a new tensor with the exponential of the elements of the input tensor.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    ```\n",
    "- `torch.sin` and `torch.cos`: Computes the sine and cosine of each element in the input tensor.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    ```\n",
    "- `torch.unsqueeze`: Adds a dimension to the tensor at the specified position.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    pe = pe.unsqueeze(0)\n",
    "    ```\n",
    "- `torch.transpose`: Swaps two dimensions of the tensor.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    pe = pe.transpose(0, 1)\n",
    "    ```\n",
    "\n",
    "- `nn.MultiheadAttention`: Allows the model to jointly attend to information from different representation subspaces.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    mha = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
    "    attn_output, attn_output_weights = mha(query, key, value)\n",
    "    ```\n",
    "\n",
    "- `torch.rand`: Generates random numbers between 0 and 1.\n",
    "  \n",
    "- `nn.Embedding`: Turns indices into dense vectors of fixed size.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    embedding = nn.Embedding(num_embeddings=1000, embedding_dim=512)\n",
    "    embedded = embedding(input_indices)\n",
    "    ```\n",
    "- `torch.triu`: Returns the upper triangular part of a matrix.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    mask = torch.triu(torch.ones(seq_length, seq_length))\n",
    "    ```\n",
    "- `torch.ones`: Creates a tensor filled with ones.\n",
    "\n",
    "- `torch.randint`: Returns a tensor filled with random integers.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    src_input = torch.randint(0, src_ntokens, (src_seq_length, batch_size))\n",
    "    ```\n",
    "\n",
    "- `nn.Linear`: Applies a linear transformation to the incoming data.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    linear_layer = nn.Linear(in_features=512, out_features=2048)\n",
    "    output = linear_layer(input_tensor)\n",
    "    ```\n",
    "- `torch.relu`: Applies the ReLU activation function element-wise.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    activated_output = torch.relu(input_tensor)\n",
    "    ```\n",
    "- `nn.Dropout`: Randomly zeroes some of the elements of the input tensor with probability `p`.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    dropout_layer = nn.Dropout(p=0.1)\n",
    "    output = dropout_layer(input_tensor)\n",
    "    ```\n",
    "- `nn.LayerNorm`: Applies Layer Normalization over a mini-batch of inputs.\n",
    "  - **Example Usage:**\n",
    "    ```python\n",
    "    layer_norm = nn.LayerNorm(normalized_shape=512)\n",
    "    normalized_output = layer_norm(input_tensor)\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating a simple dataset\n",
    "def generate_dummy_data(num_samples, seq_length, vocab_size):\n",
    "    src_data = torch.randint(0, vocab_size, (seq_length, num_samples))\n",
    "    tgt_data = torch.randint(0, vocab_size, (seq_length, num_samples))\n",
    "    return src_data, tgt_data\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000\n",
    "seq_length = 35\n",
    "vocab_size = 1000\n",
    "\n",
    "# Generate dummy data\n",
    "src_data, tgt_data = generate_dummy_data(num_samples, seq_length, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.119060516357422\n",
      "Epoch 2, Loss: 7.166744232177734\n",
      "Epoch 3, Loss: 7.092496395111084\n",
      "Epoch 4, Loss: 7.048351287841797\n",
      "Epoch 5, Loss: 6.997982501983643\n",
      "Epoch 6, Loss: 6.931274890899658\n",
      "Epoch 7, Loss: 6.921885013580322\n",
      "Epoch 8, Loss: 6.930119514465332\n",
      "Epoch 9, Loss: 6.910755634307861\n",
      "Epoch 10, Loss: 6.943057060241699\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Assuming vocab_size is already defined\n",
    "vocab_size = 1000  # Example value, set your actual vocab size here\n",
    "tgt_vocab_size = vocab_size  # Target vocabulary size\n",
    "\n",
    "# Create model instance\n",
    "model = TransformerModel(vocab_size, tgt_vocab_size, d_model=512, nhead=8,\n",
    "                         dim_feedforward=2048,\n",
    "                         num_layers=6,\n",
    "                         dropout=0.1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        src_batch = src_data[:, i:i+batch_size]\n",
    "        tgt_batch = tgt_data[:, i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src_batch, tgt_batch[:-1])  # Exclude last target token for input\n",
    "        \n",
    "        # Calculate loss (ignoring padding)\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt_batch[1:].contiguous().view(-1))\n",
    "        \n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src_input, beam_width=3, max_length=35):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialize the beam with the start token\n",
    "        beams = [(torch.tensor([start_token]), 0)]  # (sequence, score)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                if seq[-1] == end_token:  # Stop if end token is reached\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                # Get predictions from the model\n",
    "                output = model(src_input.unsqueeze(1), seq.unsqueeze(1))[-1]  # Last output token\n",
    "                \n",
    "                # Get top k predictions\n",
    "                topk_scores, topk_indices = torch.topk(output[-1], beam_width)\n",
    "                \n",
    "                for new_score, new_index in zip(topk_scores.tolist(), topk_indices.tolist()):\n",
    "                    new_seq = torch.cat((seq, torch.tensor([new_index])))\n",
    "                    new_beams.append((new_seq, score + new_score))\n",
    "\n",
    "            # Sort by score and keep only the best beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        return beams[0][0]  # Return the best sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sarra\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarra\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarra\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2d79bdc30f46728b1daac7d1002bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarra\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sarra\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c551b24f903472f9b3f42b2448654b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82fc0f2060d4937aeea32dbedc9c2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8953aa70c44f1c8c30841e3ff51a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d012e285151409983f75549ff81f068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2b215f815b471790750b789304e5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443789b67b5249bb93743c9acd3312f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Encode input text and generate output\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text with the model\n",
    "output_sequences = model.generate(input_ids=input_ids,\n",
    "                                   max_length=50,\n",
    "                                   num_return_sequences=1)\n",
    "\n",
    "# Decode generated sequences back to text\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
