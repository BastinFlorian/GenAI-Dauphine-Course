{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Avvxx6QEvo7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing the Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "snZBAKXlG0m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        ######\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        # Self-attention layer\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "\n",
        "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        ######\n",
        "\n",
        "        return src"
      ],
      "metadata": {
        "id": "xU7AlKgtE225"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "VluRgf6MKBJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the TransformerEncoderLayer\n",
        "# Parameters\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "dim_feedforward = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "# Create an instance of the TransformerEncoderLayer\n",
        "encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "\n",
        "# Dummy input data (sequence length, batch size, embedding size)\n",
        "seq_length = 10\n",
        "batch_size = 2\n",
        "dummy_input = torch.rand(seq_length, batch_size, d_model)\n",
        "\n",
        "# Forward pass\n",
        "output = encoder_layer(dummy_input)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENH3C0a-E254",
        "outputId": "c39053f3-06dc-45e1-b24d-6eba2b4f9b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([10, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the Full Transformer Encoder"
      ],
      "metadata": {
        "id": "4HRrVqLzKgQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        output = src\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, src_mask=mask,\n",
        "                         src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "V9y_9BPqE28v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pslZuFbN0w9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Transformer Encoder"
      ],
      "metadata": {
        "id": "EVSZhI7sKtKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the TransformerEncoder\n",
        "num_layers = 6\n",
        "\n",
        "# Initialize the TransformerEncoder\n",
        "transformer_encoder = TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "# Dummy input data remains the same\n",
        "# Forward pass\n",
        "output = transformer_encoder(dummy_input)\n",
        "\n",
        "print(f\"Output shape after TransformerEncoder: {output.shape}\")"
      ],
      "metadata": {
        "id": "J1Fck3rPE2_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee65fbe3-f3f7-40d4-9a0e-1c0e6f50d5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape after TransformerEncoder: torch.Size([10, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing Positional Encoding"
      ],
      "metadata": {
        "id": "SmxGXqQZK6hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "        # max_len is the maximum length of the input sequence (ie)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "xmIupjoqE3CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the PositionalEncoding"
      ],
      "metadata": {
        "id": "xT4IeNIeLYs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the PositionalEncoding\n",
        "pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "# Dummy input embeddings (sequence length, batch size, embedding size)\n",
        "dummy_embeddings = torch.zeros(seq_length, batch_size, d_model)\n",
        "\n",
        "# Forward pass\n",
        "pos_encoded_embeddings = pos_encoder(dummy_embeddings)\n",
        "\n",
        "print(f\"Positional Encoded Embeddings shape: {pos_encoded_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "8kE9BEKnE3Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e41e62-f16c-44c2-e20a-e1d3fb446996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional Encoded Embeddings shape: torch.Size([10, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assembling the Complete Transformer Model"
      ],
      "metadata": {
        "id": "9Jk1UIMgLjRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, d_model, nhead, dim_feedforward, num_layers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        #####  Initialize the weights of the model #####\n",
        "        # Hint: Initialize the embedding and decoder weights uniformly, and set decoder biases to zero.\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "rbZjEgDIE3II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example usage"
      ],
      "metadata": {
        "id": "PtRZHKfoLyxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "ntokens = 1000  # Size of vocabulary\n",
        "d_model = 512   # Embedding size\n",
        "nhead = 8       # Number of attention heads\n",
        "dim_feedforward = 2048  # Feedforward network hidden layer size\n",
        "num_layers = 6  # Number of encoder layers\n",
        "dropout = 0.2   # Dropout rate\n",
        "\n",
        "model = TransformerModel(ntokens, d_model, nhead, dim_feedforward, num_layers, dropout)\n",
        "\n",
        "# Dummy input data (sequence length, batch size)\n",
        "batch_size = 32\n",
        "seq_length = 35\n",
        "input_data = torch.randint(0, ntokens, (seq_length, batch_size))\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_data)\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plMHKvi31Ts9",
        "outputId": "b5f46a51-2fb5-4614-a0b3-c665cb597774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([35, 32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUqYNf4j1Tpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing the Transformer Decoder Layer"
      ],
      "metadata": {
        "id": "6q7CCcI7L_Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        #  Define the feedforward neural network layers\n",
        "        # Hint: Similar to the encoder layer, use two linear layers with a ReLU activation and dropout in between.\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # Self-attention layer\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "\n",
        "        #  Implement the cross-attention layer\n",
        "        # Hint: Use multihead attention where the query is the decoder input and the key and value are the encoder output.\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "\n",
        "        # Feedforward layer\n",
        "        tgt2 = self.linear2(self.dropout(torch.relu(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "u2S9-_471Tno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing the Transformer Decoder Layer2"
      ],
      "metadata": {
        "id": "6CbekP3VMUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        #  Define the feedforward neural network layers\n",
        "        # Hint: Similar to the encoder layer, use two linear layers with a ReLU activation and dropout in between.\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # Self-attention layer\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "\n",
        "        # TODO: Implement the cross-attention layer\n",
        "        # Hint: Use multihead attention where the query is the decoder input and the key and value are the encoder output.\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "\n",
        "        # Feedforward layer\n",
        "        tgt2 = self.linear2(self.dropout(torch.relu(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "f2WQIjLc1Tlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the TransformerDecoderLayer"
      ],
      "metadata": {
        "id": "y-VhOTlPMjGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the TransformerDecoderLayer\n",
        "# Parameters\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "dim_feedforward = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "# Create an instance of the TransformerDecoderLayer\n",
        "decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "\n",
        "# Dummy input data for the decoder (target sequence length, batch size, embedding size)\n",
        "tgt_seq_length = 12\n",
        "batch_size = 2\n",
        "dummy_tgt = torch.rand(tgt_seq_length, batch_size, d_model)\n",
        "\n",
        "# Dummy memory from the encoder (source sequence length, batch size, embedding size)\n",
        "memory = torch.rand(seq_length, batch_size, d_model)\n",
        "\n",
        "# Forward pass\n",
        "output = decoder_layer(dummy_tgt, memory)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buzyl1mH1Ti7",
        "outputId": "a64e3d53-86a5-4323-8c7c-d22d84639aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([12, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Full Transformer Decoder"
      ],
      "metadata": {
        "id": "I9gVgQ6WM7Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer Decoder\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        output = tgt\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                         memory_mask=memory_mask,\n",
        "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                         memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "VUikmafT1Tgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the TransformerDecoder"
      ],
      "metadata": {
        "id": "UlcXkIQaM_50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the TransformerDecoder\n",
        "num_layers = 6\n",
        "\n",
        "# Initialize the TransformerDecoder\n",
        "transformer_decoder = TransformerDecoder(decoder_layer, num_layers)\n",
        "\n",
        "# Dummy input data remains the same\n",
        "# Forward pass\n",
        "output = transformer_decoder(dummy_tgt, memory)\n",
        "\n",
        "print(f\"Output shape after TransformerDecoder: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9ZOyxdd9mDh",
        "outputId": "cab8ba1e-0858-4f56-c107-1eb39bd83b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape after TransformerDecoder: torch.Size([12, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating the Transformer Model with Decoder"
      ],
      "metadata": {
        "id": "0FjDMYK2NKrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Transformer Model with Encoder and Decoder\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_ntoken, tgt_ntoken, d_model, nhead, dim_feedforward, num_layers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.tgt_mask = None\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.pos_decoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "        decoder_layers = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers)\n",
        "\n",
        "        self.src_encoder = nn.Embedding(src_ntoken, d_model)\n",
        "        self.tgt_encoder = nn.Embedding(tgt_ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, tgt_ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the weights of the model\n",
        "        initrange = 0.1\n",
        "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate a square mask for the sequence. Mask out future positions.\"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
        "        src_emb = self.src_encoder(src) * math.sqrt(self.d_model)\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "        memory = self.transformer_encoder(src_emb, src_mask)\n",
        "\n",
        "        tgt_emb = self.tgt_encoder(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.pos_decoder(tgt_emb)\n",
        "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "FwP-DelW9l_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Complete Transformer Model with Decoder"
      ],
      "metadata": {
        "id": "sqWPcG0yNTNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "src_ntokens = 1000  # Size of source vocabulary\n",
        "tgt_ntokens = 1000  # Size of target vocabulary\n",
        "d_model = 512       # Embedding size\n",
        "nhead = 8           # Number of attention heads\n",
        "dim_feedforward = 2048  # Feedforward network hidden layer size\n",
        "num_layers = 6      # Number of encoder and decoder layers\n",
        "dropout = 0.2       # Dropout rate\n",
        "\n",
        "model = TransformerModel(src_ntokens, tgt_ntokens, d_model, nhead, dim_feedforward, num_layers, dropout)\n",
        "\n",
        "# Dummy input data\n",
        "batch_size = 32\n",
        "src_seq_length = 35\n",
        "tgt_seq_length = 30\n",
        "src_input = torch.randint(0, src_ntokens, (src_seq_length, batch_size))\n",
        "tgt_input = torch.randint(0, tgt_ntokens, (tgt_seq_length, batch_size))\n",
        "\n",
        "# Generate masks\n",
        "# TODO: Generate the target mask to prevent the decoder from attending to future positions\n",
        "# Hint: Use the generate_square_subsequent_mask method provided in the model\n",
        "tgt_mask = model.generate_square_subsequent_mask(tgt_seq_length)\n",
        "\n",
        "# Forward pass\n",
        "output = model(src_input, tgt_input, tgt_mask=tgt_mask)\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0TemRqy9l9k",
        "outputId": "2efef556-6600-408f-9cc9-60449b75cf03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([30, 32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8H5li1S-J7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEY8If1D-J3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2a9fOMCt-J2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YrhKVQRl-Jz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2eymQsrk9l7R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
