{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Attention?\n",
    "\n",
    "Attention mechanisms allow models to focus on relevant parts of the input when generating each part of the output. In sequence-to-sequence tasks like machine translation, attention helps the model decide which words in the input sentence are most relevant to generating the next word in the output sentence.\n",
    "\n",
    "### Why is Attention Important?\n",
    "\n",
    "- **Handles Long Dependencies**: Captures relationships between distant elements in sequences.\n",
    "- **Improves Performance**: Enhances accuracy in tasks like translation, summarization, and question answering.\n",
    "- **Interpretability**: Provides insights into what the model is focusing on, making it more interpretable.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Query (Q)**: The vector representing the current focus point.\n",
    "- **Keys (K)**: The vectors representing all possible focus points.\n",
    "- **Values (V)**: The vectors containing the information to be aggregated.\n",
    "\n",
    "### Attention Calculation:\n",
    "\n",
    "1. **Score Calculation**: Compute a score between the query and each key.\n",
    "   - Example: Dot product between Q and K.\n",
    "2. **Weighting**: Apply a softmax function to obtain weights.\n",
    "   - Ensures that the weights sum to 1.\n",
    "3. **Aggregation**: Multiply the weights with the values and sum them up.\n",
    "\n",
    "**Attention:**\n",
    "\n",
    "<img src=\"../docs/attention.png\" alt=\"Alt Text\" width=\"500\"/>\n",
    "\n",
    "\n",
    "**Softmax:**\n",
    "\n",
    "<img src=\"../docs/softmax.png\" alt=\"Alt Text\" width=\"500\"/>\n",
    "\n",
    "\n",
    "**In this section:**\n",
    "\n",
    "- You'll implement the scaled dot-product attention mechanism from scratch.\n",
    "- Understand each step of the computation.\n",
    "- Verify the implementation with test cases.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Compute the scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        q: Queries tensor of shape (batch_size, seq_length_q, d_k)\n",
    "        k: Keys tensor of shape (batch_size, seq_length_k, d_k)\n",
    "        v: Values tensor of shape (batch_size, seq_length_v, d_v)\n",
    "        mask: Optional mask tensor to prevent attention to certain positions.\n",
    "\n",
    "    Returns:\n",
    "        context: The result of the attention mechanism.\n",
    "        attention_weights: The weights assigned to each value.\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # Apply mask, if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Compute the context vector as the weighted sum of values\n",
    "    context = torch.matmul(attention_weights, v)\n",
    "\n",
    "    return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape: torch.Size([2, 5, 64])\n",
      "Attention weights shape: torch.Size([2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Test the scaled_dot_product_attention function\n",
    "batch_size = 2\n",
    "seq_length_q = 5\n",
    "seq_length_k = 6\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "# Random tensors for queries, keys, and values\n",
    "q = torch.rand(batch_size, seq_length_q, d_k)\n",
    "k = torch.rand(batch_size, seq_length_k, d_k)\n",
    "v = torch.rand(batch_size, seq_length_k, d_v)\n",
    "\n",
    "mask = None\n",
    "\n",
    "# Compute attention\n",
    "context, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "print(f\"Context shape: {context.shape}\")  # Expected: (batch_size, seq_length_q, d_v)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")  # Expected: (batch_size, seq_length_q, seq_length_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention \n",
    "\n",
    "Multi-head attention allows the model to focus on different positions and represent the subspaces differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.q_linear(q)  # (batch_size, seq_length, d_model)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # TODO: Split into num_heads\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  \n",
    "        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "       \n",
    "        context, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.out_linear(context)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadAttention class\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "\n",
    "# Random input tensor\n",
    "x = torch.rand(batch_size, seq_length, d_model)\n",
    "\n",
    "# Instantiate the MultiHeadAttention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  \n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Attention Weights\n",
    "\n",
    "Visualization helps in understanding which parts of the input the model is focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_attention_weights(attention_weights, layer=0, head=0):\n",
    "    \"\"\"\n",
    "    Plots the attention weights.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: Tensor of attention weights of shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        layer: The layer number (if applicable)\n",
    "        head: The head number to visualize\n",
    "    \"\"\"\n",
    "    # Select the first batch\n",
    "    attn = attention_weights[0, head].detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn, cmap='viridis')\n",
    "    plt.title(f'Attention Weights - Layer {layer+1}, Head {head+1}')\n",
    "    plt.xlabel('Key Positions')\n",
    "    plt.ylabel('Query Positions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Assume we have attention weights from the previous example\n",
    "plot_attention_weights(attn_weights, layer=0, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
