{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn‚Äôt efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI, hub\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Charge les variables depuis .env\n",
    "#print(os.getenv(\"GOOGLE_API_KEY\"))  # V√©rifiez si la cl√© est bien charg√©e\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "üë®üèº‚Äçüéì Master MASH - Universit√© PSL\n",
      "üë®üèº‚Äçüíª LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "1\n",
      "\n",
      "2\n",
      "Module Overview:\n",
      "\n",
      "Building Large Language Models\n",
      "Transformers\n",
      "Retrieval Augmented Generation\n",
      "Tools and Agents\n",
      "Fine tuning and optimization techniques\n",
      "Generative AI in vision\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load the document\n",
    "filename = \"../data/gen_ai_course.txt\"\n",
    "loader = TextLoader(filename, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Optionally, print the first chunk to verify\n",
    "print(docs[0].page_content[:500])  # Show the first 500 characters of the first chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "# Generate embeddings for each document chunk\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings_list = embeddings.embed_documents([doc.page_content for doc in docs])\n",
    "\n",
    "# Build the FAISS vector store from the embeddings\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Generative AI\n",
      "Answer: Generative AI refers to a category of artificial intelligence algorithms that can create various types of content, including text, images, audio, and synthetic data.  These models learn the underlying patterns and structure of their input training data and then generate new data that has similar characteristics.  Generative AI is used in a wide range of applications, from creating art and music to writing articles and generating code.  It relies heavily on techniques like deep learning, particularly generative adversarial networks (GANs) and transformer models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a RetrievalQA chain\n",
    "# Hint: Use ChatOpenAI, create a prompt, and use StrOutputParser\n",
    "# Hint: The chain should be an LCEL chain https://python.langchain.com/v0.1/docs/expression_language/get_started/\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a question answering chatbot. \n",
    "            You'll say if you don't know. \n",
    "            You'll find the relevant information in {formatted_docs}. \n",
    "            Answer in at most 5 sentences.\"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "question = \"What is Generative AI\"\n",
    "\n",
    "\n",
    "formatted_docs = format_docs(docs)\n",
    "\n",
    "qa_chain = prompt | llm\n",
    "answer = qa_chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"English\",\n",
    "        \"query\": question,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ").content\n",
    "\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Attention Mechanism\n",
      "Answer: The attention mechanism in Transformers allows the model to focus on different parts of the input sequence when generating each word of the output sequence.  It does this by calculating a weighted sum of the input values, where the weights are determined by the relevance of each input word to the current output word being generated.  This relevance is determined by comparing a query vector (representing the current output word) with key vectors (representing each input word), and then using a softmax function to normalize the resulting scores into weights.  This allows the model to prioritize the most relevant input words when generating the output.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# Answer:\n",
    "query = \"What is Attention Mechanism\"\n",
    "result = qa_chain.invoke(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ").content\n",
    "print(f\"Question: {query}\\nAnswer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention calculates relationships between different parts of a single input sequence to better understand the overall context.  Multi-head attention runs self-attention multiple times in parallel with different learned linear projections (heads).  Each head focuses on different aspects of the input, allowing the model to capture a richer representation. The results from each head are then concatenated and linearly transformed to produce the final output.  This allows the model to jointly attend to information from different representation subspaces at different positions.  In summary, multi-head attention enhances self-attention by allowing the model to learn diverse relationships within the input sequence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"Can you expalain the difference between Self Attention and Multi Head Attention\"\n",
    "result = qa_chain.invoke(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You‚Äôve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
