{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isnâ€™t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI, hub\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from typing import List\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='C:/IAGen/exercices/.env')\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "if google_api_key is None or db_password is None:\n",
    "    print(\"Error: One or more environment variables are not loaded.\")\n",
    "else:\n",
    "    print(\"Environment variables loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "filename = r\"C:\\IAGen\\data\\gen_ai_course.txt\"\n",
    "# Answer:\n",
    "loader = TextLoader(filename, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# Answer\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to train an LLM ?\n",
      "Answer: LLM training involves two phases: pre-training and post-training. Pre-training uses a massive text dataset and cross-entropy loss to predict the next word in a sequence, teaching the model language structure and semantics. Post-training refines the model's ability to follow instructions and align with human preferences.  Supervised fine-tuning (SFT) uses human-written examples to guide the model's responses. Reinforcement learning from human feedback (RLHF) further improves alignment by training a reward model based on human preferences and using it to optimize the LLM's output. Direct preference optimization (DPO) is a newer, simpler alternative to RLHF that directly optimizes the model based on human preferences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a RetrievalQA chain\n",
    "# Hint: Use ChatOpenAI, create a prompt, and use StrOutputParser\n",
    "# Hint: The chain should be an LCEL chain https://python.langchain.com/v0.1/docs/expression_language/get_started/\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a question answering chatbot. \n",
    "            You'll say if you don't know. \n",
    "            You'll find the relevant information in {formatted_docs}. \n",
    "            Answer in at most 5 sentences.\"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "question = \"How to train an LLM ?\"\n",
    "\n",
    "\n",
    "formatted_docs = format_docs(docs)\n",
    "\n",
    "# Answer:\n",
    "qa_chain = prompt | llm\n",
    "answer = qa_chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\" or \"French\",\n",
    "        \"output_language\": \"English\",\n",
    "        \"query\": question,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ").content\n",
    "\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's an encoder \n",
      "Answer: An encoder is a component of a transformer model that processes input sequences, such as text or images, and transforms them into a contextualized representation. It uses self-attention mechanisms to capture relationships between different parts of the input sequence, allowing the model to understand the overall meaning and context.  The encoder's output is a sequence of vectors, where each vector represents a part of the input and its relationship to other parts. In tasks like machine translation, the encoder's output is then passed to a decoder, which generates the translated sequence.  In other tasks, like text classification, the encoder's output can be directly used for prediction.\n",
      "\n",
      "An encoder is a component of a transformer model that processes input sequences, such as text or images, and transforms them into a contextualized representation. It uses self-attention mechanisms to capture relationships between different parts of the input sequence, allowing the model to understand the overall meaning and context.  The encoder's output is a sequence of vectors, where each vector represents a part of the input and its relationship to other parts. In tasks like machine translation, the encoder's output is then passed to a decoder, which generates the translated sequence.  In other tasks, like text classification, the encoder's output can be directly used for prediction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# Answer:\n",
    "query = \"What's an encoder? \"\n",
    "result = qa_chain.invoke(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ").content\n",
    "print(f\"Question: {query}\\nAnswer: {result}\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florian Bastin holds a Master MASH degree from UniversitÃ© PSL and works as an LLM Engineer at OctoTechnology.  He has collaborated with prominent organizations such as Le Monde, Casino, Channel, Club Med, Pernod Ricard, and Suez.  His presentation, \"Generative AI with LLM,\" covers topics including building and fine-tuning large language models, transformers, retrieval augmented generation, and generative AI in vision.  He discusses pre-training and post-training phases, including supervised fine-tuning and RLHF, along with various optimization techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"who is Florian Bastin ?\"\n",
    "result = qa_chain.invoke(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"formatted_docs\": formatted_docs,\n",
    "    }\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! Youâ€™ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
