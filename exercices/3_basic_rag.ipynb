{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "filename = r\"C:\\Users\\Aycha\\Desktop\\M2_BDIA\\GENAI\\GenAI-Dauphine-Course\\data\\gen_ai_course.txt\"\n",
    "# Answer:\n",
    "loader = TextLoader(filename, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# Answer\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embeddings and store them in a VectorStore\n",
    "# Hint: Use OpenAIEmbeddings and FAISS\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "# Hint : Use GoogleGenerativeAIEmbeddings(model=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aycha\\AppData\\Local\\Temp\\ipykernel_15008\\2849301078.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  qa_chain = LLMChain(prompt=prompt, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "# Initialize ChatGoogleGenerativeAI with the required arguments\n",
    "\n",
    "#Create a function to format documents for the prompt\n",
    "def format_docs(docs: List[Document]):\n",
    "    # Hint: Join the content of each document\n",
    "    return \"\\n\".join(doc.page_content for doc in docs) # Join the page content of docs into a stringg\n",
    "\n",
    "# Hint: Define the prompt template with system and human messages. See help below\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided documents.\"),\n",
    "    (\"human\", \"Here are some relevant documents to answer the question.\"),\n",
    "    (\"human\", \"{documents}\"),\n",
    "    (\"human\", \"User's question: {question}\"),\n",
    "    (\"assistant\", \"Answer:\"),\n",
    "])\n",
    "# Hint: Format the documents using the function above\n",
    "formatted_docs = format_docs(docs) \n",
    "\n",
    "# Hint: Create the QA chain by combining the prompt and model\n",
    "qa_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aycha\\AppData\\Local\\Temp\\ipykernel_15008\\1237972392.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain.run(documents=formatted_docs, question=query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document primarily discusses Generative AI with Large Language Models (LLMs), covering topics such as:\n",
      "\n",
      "* **Building LLMs:** including pretraining, tokenization, evaluation, data preprocessing, scaling laws, training processes, costs, and optimization.  It also covers fine-tuning with Supervised Fine Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), reward models, Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO).\n",
      "* **Transformers:** the core architecture behind LLMs.  The document details the history leading up to transformers (N-grams, embeddings, RNNs, LSTMs), and then explains the key components of the transformer architecture itself (self-attention, multi-head attention, residual connections, layer normalization, feed-forward layers, softmax, and positional embeddings).\n",
      "* **Retrieval Augmented Generation (RAG):** This section explains how RAG enhances LLMs by connecting them to external knowledge sources.  It covers information retrieval methods, vector databases, search optimization, different RAG techniques (query augmentation, rephrasing, etc.), evaluation, multimodal RAG, and state-of-the-art RAG architectures.\n",
      "* **Tools and Agents:**  While less detailed, the document touches upon how tools and agents can be used with LLMs.\n",
      "\n",
      "\n",
      "The most in-depth discussions revolve around the architecture and training of LLMs, particularly transformers, and the methods and advancements within RAG.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# Answer:\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document covers several key aspects of Large Language Models (LLMs), including:\n",
      "\n",
      "**Building LLMs:**\n",
      "* **Pretraining:** This phase uses autoregressive models, cross-entropy loss, tokenization (like BPE), and perplexity for evaluation. Data preprocessing is crucial, involving cleaning, deduplication, and filtering. Scaling laws guide model and data size based on compute resources.  Training involves tuning hyperparameters on smaller models before scaling up. Cost and optimization are major considerations.\n",
      "* **Fine-tuning:** Aligning LLMs to follow instructions is done through Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF).  SFT uses human-labeled data to improve instruction following, while RLHF uses a reward model and algorithms like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO) to align with human preferences. Evaluation remains a challenge. Chain-of-Thought (COT) prompting improves reasoning abilities.\n",
      "\n",
      "**Transformers:** The core architecture of LLMs. Key components include:\n",
      "* **Self-Attention/Cross-Attention:** Mechanisms that allow the model to weigh the importance of different parts of the input sequence. Multi-head attention allows for parallel processing of these relationships.\n",
      "* **Other components:** Residual connections, layer normalization, feed-forward layers, softmax layer, and positional embeddings.  These work together to enable efficient training and effective representation of sequential data. Pre-transformer architectures like N-grams, RNNs, and LSTMs are also discussed.\n",
      "\n",
      "**Retrieval Augmented Generation (RAG):** This technique enhances LLMs by connecting them to external knowledge sources. Key aspects include:\n",
      "* **Information Retrieval:**  Methods like TF-IDF, BM25, cosine similarity, and Maximal Marginal Relevance (MMR) are used for retrieving relevant information.  Sparse and dense retrieval methods are discussed, along with hybrid approaches.\n",
      "* **Vectorstores:**  Specialized databases for storing and querying vector embeddings of documents.  Efficient similarity search methods like ScaNN, FAISS, and HNSW are important.\n",
      "* **RAG techniques:** Query augmentation, query rephrasing, context management, chunking, and prompt engineering.  Evaluation methods and multimodal RAG are also covered. Advanced RAG architectures like Self-RAG, RAPTOR, Corrective RAG, and GraphRAG are introduced.\n",
      "\n",
      "**Tools and Agents:** LLMs can be augmented with tools to access and process external information, and agents can use these tools to perform complex tasks.\n",
      "\n",
      "\n",
      "The document emphasizes the importance of data quality, compute resources, and alignment with human preferences in building effective LLMs.  It also highlights the evolution of LLM architectures and the growing importance of techniques like RAG for enhancing their capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"Can you summarize the key points mentioned?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing config: chunk_size=300, chunk_overlap=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aycha\\AppData\\Local\\Temp\\ipykernel_15008\\826605874.py:19: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing config: chunk_size=500, chunk_overlap=100\n",
      "Testing config: chunk_size=200, chunk_overlap=50\n"
     ]
    }
   ],
   "source": [
    "chunk_configs = [\n",
    "    {\"chunk_size\": 300, \"chunk_overlap\": 100},\n",
    "    {\"chunk_size\": 500, \"chunk_overlap\": 100},\n",
    "    {\"chunk_size\": 200, \"chunk_overlap\": 50},\n",
    "]\n",
    "\n",
    "for config in chunk_configs:\n",
    "    print(f\"Testing config: chunk_size={config['chunk_size']}, chunk_overlap={config['chunk_overlap']}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config[\"chunk_size\"], \n",
    "        chunk_overlap=config[\"chunk_overlap\"]\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "    query = \"What are the key points discussed in the documents?\"\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    results = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicha_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
