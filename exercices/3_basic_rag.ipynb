{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We first need to import all the necessary modules and set up the environment. The environment variable for the GOOGLE_API_KEY is loaded from the .env file to interact with Google’s Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Load the Google API key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "filename = \"../data/gen_ai_course.txt\"\n",
    "# Answer:\n",
    "loader = TextLoader(filename, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Answer\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval.\n",
    "\n",
    "We now generate embeddings for each chunk of text using GoogleGenerativeAIEmbedding, which is an effective and efficient way to create vector representations of text. The embeddings are then stored in a FAISS index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use OpenAIEmbeddings and FAISS\n",
    "# Utiliser GoogleGenerativeAIEmbeddings pour créer les embeddings des documents\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Construire le VectorStore avec FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them.\n",
    "\n",
    "The next step is to set up a RetrievalQA chain that combines the information retrieval process with the language generation model. The chain will search for the most relevant documents and pass them to Gemini for question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du prompt structuré avec ChatPromptTemplate\n",
    "prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"),\n",
    "    (\"human\", \"Question: {question}\"),\n",
    "    (\"human\", \"Context: {context}\"),\n",
    "    (\"ai\", \"Answer:\")\n",
    "])\n",
    "\n",
    "# Fonction pour formater les documents récupérés en texte brut\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Fonction pour interroger Gemini avec les documents récupérés via FAISS\n",
    "def query_to_gemini(query: str, top_k: int = 3):\n",
    "    # Recherche des documents les plus proches dans le vectorstore FAISS\n",
    "    docs_with_score = vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # docs_with_score contient maintenant une liste de documents (et non un couple doc, score)\n",
    "    relevant_docs = docs_with_score  # Nous pouvons directement utiliser la liste de documents\n",
    "    \n",
    "    # Formater les documents pour le prompt\n",
    "    formatted_docs = format_docs(relevant_docs)\n",
    "    \n",
    "    # Créer un prompt structuré avec les documents pertinents et la question\n",
    "    formatted_prompt = prompt.format_messages(question=query, context=formatted_docs)\n",
    "    \n",
    "    # Interroger Gemini via LangChain pour obtenir une réponse\n",
    "    ai_msg = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Vérifier si le message renvoyé par Gemini contient la réponse\n",
    "    if hasattr(ai_msg, 'content'):\n",
    "        reponse= ai_msg.content\n",
    "        reponse = reponse.replace(\". \", \".\\n\")  # Remplacer chaque point suivi d'un espace par un saut de ligne\n",
    "        return reponse\n",
    "    else:\n",
    "        return \"Erreur : la réponse générée ne contient pas de texte valide.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse : The main topic is Retrieval Augmented Generation (RAG), a technique that uses retrieved information to improve model responses.\n",
      " The document discusses RAG's architecture, including information retrieval methods and filtering techniques, to enhance context and coherence.\n",
      " It also briefly touches upon TF-IDF and Elasticsearch for text search within the RAG framework.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = query_to_gemini(query)\n",
    "print(f\"Réponse : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse : Retrieval Augmented Generation (RAG) uses a vector store of document content to improve model responses.\n",
      " Key improvements to RAG include context enrichment and multi-faceted filtering.\n",
      " Transformers use self/cross attention, multi-head attention, residual connection, layer normalization, a feed forward layer, softmax layer, and positional embeddings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you summarize the key points mentioned?\"\n",
    "result = query_to_gemini(query)\n",
    "print(f\"Réponse : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used Sentence-Transformers \n",
    "\n",
    "In this part we will Sentence-Transformers to create embeddings from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Utiliser le modèle `all-MiniLM-L6-v2` qui est léger et efficace pour des embeddings de documents\n",
    "model1 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Générer les embeddings pour chaque document\n",
    "embeddings1 = model1.encode([doc.page_content for doc in docs])\n",
    "\n",
    "# Convertir les embeddings en numpy array pour FAISS\n",
    "embeddings_np = np.array(embeddings1).astype('float32')\n",
    "\n",
    "# Créer un index FAISS\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance (distance euclidienne)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "\n",
    "# 4. Fonction pour interroger le modèle Gemini avec les documents récupérés\n",
    "def query_to_gemini1(query: str, top_k: int = 3):\n",
    "    # Générer l'embedding de la requête\n",
    "    query_embedding = model1.encode([query]).astype('float32')\n",
    "\n",
    "    # Recherche des documents les plus proches dans l'index FAISS\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Récupérer les documents les plus proches\n",
    "    relevant_docs = [docs[i] for i in I[0]]\n",
    "\n",
    "    # Créer un prompt structuré avec les documents pertinents\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that answers questions based on the provided documents.\"),\n",
    "        (\"user\", f\"Here are some documents: {format_docs(relevant_docs)}\\n\\nQuestion: {query}\")\n",
    "    ])\n",
    "\n",
    "    # Formater le prompt pour l'API LangChain\n",
    "    formatted_prompt = prompt.format_messages(query=query)\n",
    "\n",
    "    # Interroger Gemini via LangChain pour obtenir une réponse\n",
    "    ai_msg = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # Vérifier si le message renvoyé par Gemini contient la réponse\n",
    "    if hasattr(ai_msg, 'content'):\n",
    "        return ai_msg.content\n",
    "    else:\n",
    "        return \"Erreur : la réponse générée ne contient pas de texte valide.\"\n",
    "    \n",
    "# 5. Fonction pour formater les documents à afficher dans le prompt Gemini\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Answer:\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = query_to_gemini(query)\n",
    "print(f\"result : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you summarize the key points mentioned?\"\n",
    "result = query_to_gemini(query)\n",
    "print(f\"Réponse : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
