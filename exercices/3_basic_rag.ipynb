{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI, hub\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "filename = \"../data/gen_ai_course.txt\"\n",
    "# Answer:\n",
    "loader = TextLoader(filename)  \n",
    "documents = loader.load() \n",
    "\n",
    "# Answer:\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embeddings and store them in a VectorStore\n",
    "# Hint: Use OpenAIEmbeddings and FAISS\n",
    "# Create embeddings and store them in a VectorStore\n",
    "# Answer:  \n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "#vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Initialise les embeddings avec la clé API chargée\n",
    "embeddings = GoogleGenerativeAIEmbeddings(api_key=os.getenv(\"GOOGLE_API_KEY\"), model=\"models/embedding-001\")\n",
    "\n",
    "# Créez le VectorStore avec FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a RetrievalQA chain\n",
    "# Hint: Use ChatOpenAI, create a prompt, and use StrOutputParser\n",
    "# Hint: The chain should be an LCEL chain https://python.langchain.com/v0.1/docs/expression_language/get_started/\n",
    "#llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "#prompt = ChatPromptTemplate.from_template(\n",
    "    #template=\"Based on the following documents, answer the user's question.\\n\\nDocuments:\\n{docs}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "#)\n",
    "\n",
    "#def format_docs(docs: List[Document]):\n",
    " #   return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Answer:\n",
    "#qa_chain = hub.rlm.rag_chain(\n",
    " #   retriever=vectorstore.as_retriever(),  \n",
    " #   llm=llm,  \n",
    " #   prompt=prompt,  \n",
    "  #  doc_formatter=format_docs, \n",
    "   # output_parser=StrOutputParser() \n",
    "#)  \n",
    "\n",
    "#Gemini\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    max_tokens=None,\n",
    "    timeout=20,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a question-answering chatbot. Provide answers based on the provided documents.\"),\n",
    "        (\"human\", \"Question: {question}\\n\\nRelevant Documents:\\n{formatted_docs}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_docs = format_docs(docs)\n",
    "\n",
    "qa_chain = prompt | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document provides a comprehensive overview of Large Language Models (LLMs), focusing on their architecture, training, and applications, especially Retrieval Augmented Generation (RAG). It covers topics like transformer models, pre-training and fine-tuning techniques (including RLHF and DPO), and different RAG architectures.  Additionally, it touches upon tools and agents used with LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# Answer:\n",
    "#query = \"What is the main topic discussed in the document?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "\n",
    "query_1 = \"What is the main topic discussed in the document?\"\n",
    "result = qa_chain.invoke({\n",
    "    \"question\": query_1,\n",
    "    \"formatted_docs\": formatted_docs\n",
    "}).content\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document covers many aspects of Large Language Models (LLMs), from their underlying architecture and training process to advanced techniques like Retrieval Augmented Generation (RAG) and the use of tools and agents.  Here's a breakdown of the key points:\n",
      "\n",
      "**I. Building Large Language Models (LLMs)**\n",
      "\n",
      "* **Transformers:** The core architecture replacing older models like RNNs and LSTMs, relying heavily on attention mechanisms.  Key components include self-attention, multi-head attention, residual connections, layer normalization, feed-forward layers, softmax, and positional embeddings.\n",
      "* **Pretraining:**  Involves training on massive datasets (e.g., Common Crawl) using cross-entropy loss and techniques like byte-pair encoding (BPE) for tokenization.  Scaling laws dictate the relationship between model size, data size, and compute resources.  Evaluation is done through perplexity and specialized datasets.  Data preprocessing is crucial for quality.\n",
      "* **Fine-tuning:** Aligning the model to follow instructions and user preferences.  Supervised fine-tuning (SFT) uses human-generated responses.  Reinforcement Learning from Human Feedback (RLHF) uses a reward model and algorithms like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO).\n",
      "\n",
      "**II. Retrieval Augmented Generation (RAG)**\n",
      "\n",
      "* **Architecture:** Combines LLMs with external knowledge sources to improve factual accuracy and handle information beyond the training data.  Involves document ingestion, retrieval, and contextualized answering.\n",
      "* **Information Retrieval:**  Uses methods like TF-IDF, BM25, cosine similarity, and Maximal Marginal Relevance (MMR) to find relevant documents.  Distinguishes between sparse and dense retrieval methods.  Hybrid approaches and techniques like SPLADE and DRAGON are also discussed.\n",
      "* **Vectorstores & Search Optimization:** Vector databases efficiently store and query embeddings.  Techniques like Scalable Nearest Neighbors (ScaNN), Facebook AI Similarity Search (FAISS), and Hierarchical Navigable Small Worlds (HNSW) optimize similarity search.  Rerankers and Reciprocal Rank Fusion (RRF) further refine retrieval results.\n",
      "* **RAG Techniques:** Includes query augmentation, query rephrasing, addressing \"Lost in the Middle\" context issues, prompt engineering, document loaders, chunking for long contexts, and available frameworks like Langchain and LlamaIndex.\n",
      "* **Evaluation:**  Retriever evaluation uses precision, recall, and NDCG.  Answer evaluation can leverage LLMs as judges.\n",
      "* **Multimodal RAG:** Extends RAG to handle multiple data types (text, images, etc.) using multimodal LLMs and techniques like Multimodal Rotary Position Embedding (M-ROPE).\n",
      "* **SOTA Architectures:** Discusses advanced RAG architectures like Self-RAG, RAPTOR, Corrective RAG (CRAG), and GraphRAG.\n",
      "\n",
      "**III. Tools and Agents**\n",
      "\n",
      "* **Tools:**  LLMs can be augmented with external tools to expand their capabilities (e.g., Toolformer).\n",
      "* **Agents:**  (Not detailed in the summary, but mentioned as part of the module overview.)\n",
      "\n",
      "The document emphasizes the importance of compute resources, data quality, and alignment techniques in building effective LLMs.  It also highlights the shift towards more sophisticated RAG methods for accessing and utilizing external knowledge, along with the increasing importance of multimodal capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"Can you summarize the key points mentioned?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "result = qa_chain.invoke({  \n",
    "    \"question\": query,      \n",
    "    \"formatted_docs\": formatted_docs  \n",
    "}).content\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture replaces recurrent networks with a self-attention mechanism.  Key components include:\n",
      "\n",
      "* **Self-Attention/Cross Attention:** This mechanism allows the model to weigh the importance of different parts of the input sequence when generating output.  Self-attention focuses on relationships within a single sequence, while cross-attention relates two different sequences.\n",
      "* **Multi-Head Attention:**  This performs multiple attention calculations in parallel, allowing the model to capture different relationships between words.  These parallel calculations are then combined.\n",
      "* **Residual Connections & Layer Normalization:** Residual connections help mitigate vanishing gradients and keep information localized within layers. Layer normalization stabilizes hidden state dynamics during training.\n",
      "* **Feed Forward Layer:** This layer applies two linear transformations with a ReLU activation in between to each position of the sequence separately and identically. It consists of \"up projection\" and \"down projection\" steps.\n",
      "* **Softmax Layer:** This layer converts the decoder output into probabilities for the next token in the sequence.\n",
      "* **Positional Embeddings:** Since Transformers don't process sequentially, positional encodings are added to the input embeddings to provide information about word order.  These encodings often use sinusoidal functions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"Can you summarize the Transformer's Architecture?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "result = qa_chain.invoke({  \n",
    "    \"question\": query_2,      \n",
    "    \"formatted_docs\": formatted_docs  \n",
    "}).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main difference between a Transformer and an LSTM lies in how they process sequential data:\n",
      "\n",
      "* **LSTMs** process data sequentially, maintaining a \"memory\" of past information using a cell state and gates to control information flow.  This sequential nature makes them slower for long sequences and susceptible to vanishing gradients, although LSTMs mitigate this better than standard RNNs.\n",
      "\n",
      "* **Transformers** process data in parallel using self-attention, allowing them to consider all words in a sequence simultaneously and capture relationships between distant words more effectively. This parallel processing makes them significantly faster than LSTMs, especially for long sequences, and avoids the vanishing gradient problem.  However, the computational cost of self-attention grows quadratically with sequence length, posing challenges for very long sequences.  Transformers rely on positional embeddings to incorporate sequence order information, which LSTMs inherently capture through their sequential processing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"What is the main difference between a Transformer and an LSTM ?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "result = qa_chain.invoke({  \n",
    "    \"question\": query_3,      \n",
    "    \"formatted_docs\": formatted_docs  \n",
    "}).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
