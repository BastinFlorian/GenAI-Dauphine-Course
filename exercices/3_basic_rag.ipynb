{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/gen_ai_course.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyA0BJ-l4g5TYK-Gd0fvK6lJMUIroDsr1rI'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context} \n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "retriever = vectorstore.as_retriever() #pour récupérer les documents pertinents.\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever= retriever,  \n",
    "    chain_type_kwargs={\"prompt\": prompt},  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's a Retrieval Augmented Generation?\n",
      "Answer: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances language models by giving them access to external knowledge bases or documents during generation. This allows the model to generate more accurate and up-to-date information by retrieving relevant data, rather than relying only on its internal parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What's a Retrieval Augmented Generation?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "formatted_docs = format_docs(docs)\n",
    "\n",
    "answer = qa_chain.run(\n",
    "    {\n",
    "        \"query\": question,\n",
    "        \"context\": formatted_docs,\n",
    "    })\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic discussed in the document?\n",
      "Answer: Retrieval Augmented Generation (RAG)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = qa_chain.run(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"context\": formatted_docs,\n",
    "    })\n",
    "print(f\"Question: {query}\\nAnswer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic discussed in the document?\n",
      "Answer: Retrieval Augmented Generation (RAG) is the main topic, including its basic architecture, information retrieval techniques, and enhancements like context enrichment and multi-faceted filtering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "result = result[\"result\"]\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you summarize the key points about Trasnformers?\n",
      "Answer: Transformers use an encoder-decoder architecture.  Word embeddings are used to represent words as numerical vectors, capturing semantic relationships and providing context.  These embeddings can be generated from one-hot encodings.  The model aims to handle long sequences and considers the relative positions of words.  Training efficiency is a goal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you summarize the key points about Trasnformers?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "result = result[\"result\"]\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a Trasnformer?\n",
      "Answer: Word embeddings are used to transform words into numerical values (vectors) providing semantic relationships between them.  One-hot encoding is also mentioned but embeddings are favored due to their ability to capture semantic meaning.  The model uses these embeddings to understand the context of a sentence.  The example shows how the word \"Transformers\" is embedded and how its position in the sentence contributes to its meaning (\"at the beginning of the sentence\").  The model aims to be fast trainable, though specific training methods are not detailed.  The architecture described involves a feed-forward layer and multi-head attention mechanisms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a Trasnformer?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "result = result[\"result\"]\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the steps of self-attention mechanism?\n",
      "Answer: Based on the provided text, the steps of the self-attention mechanism are not explicitly listed as ordered steps. However, the following concepts and calculations are associated with it:\n",
      "\n",
      "1. **Components:** Query (What am I looking for?), Key (What do I have?), and Value (What do I reveal to others?).\n",
      "\n",
      "2. **Calculations with Values (V):**  The text shows examples of calculations involving values (V1, V2, V3, V4) being multiplied by different weights.  For example: `1x V1`, `0.97 x V1`, `0.33 x V2`, etc.  These calculations appear to be weighted sums of the values.\n",
      "\n",
      "3. **Method 1 WV:**  A method involving multiplication with a matrix \"WV\" and subsequent calculations, including additions and multiplications with the values (V).\n",
      "\n",
      "4. **Softmax:** A softmax operation is applied to a set of numbers (4, 70, 0, 85, -4, -10, 0, 0, 2, 0, 0, 0, 3, -3, 4, 5), dividing each by 128.  This suggests softmax is part of the process.\n",
      "\n",
      "The exact sequence and relationship between these elements are not fully defined within the provided text.  It mentions \"Step 2\" regarding the number of computations, but \"Step 1\" is not described.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the steps of self-attention mechanism?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "result = result[\"result\"]\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
