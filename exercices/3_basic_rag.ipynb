{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- Sentence-Transformers: Used to create embeddings from text without relying on OpenAI.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We first need to import all the necessary modules and set up the environment. The environment variable for the GOOGLE_API_KEY is loaded from the .env file to interact with Google’s Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Charger la clé API de Google\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialisation de Gemini via LangChain\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",  # Utilisez le modèle Gemini adéquat\n",
    "    temperature=0.0,\n",
    "    max_tokens=150,\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks.\n",
    "\n",
    "Here, we load the course chapters from a .txt file and split the text into smaller chunks using RecursiveCharacterTextSplitter. This helps us manage large documents efficiently for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "# Charger et diviser le document\n",
    "filename = \"../data/gen_ai_course.txt\"  # Remplacer par le chemin du fichier\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "\n",
    "# Diviser le document en morceaux plus petits\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval.\n",
    "\n",
    "We now generate embeddings for each chunk of text using Sentence-Transformers, which is an effective and efficient way to create vector representations of text. The embeddings are then stored in a FAISS index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embeddings and store them in a VectorStore\n",
    "# Hint: Use OpenAIEmbeddings and FAISS\n",
    "\n",
    "# 3. Création des embeddings avec Sentence-Transformers\n",
    "# Utiliser le modèle `all-MiniLM-L6-v2` qui est léger et efficace pour des embeddings de documents\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Générer les embeddings pour chaque document\n",
    "embeddings = model.encode([doc.page_content for doc in docs])\n",
    "\n",
    "# Convertir les embeddings en numpy array pour FAISS\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Créer un index FAISS\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance (distance euclidienne)\n",
    "index.add(embeddings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks.\n",
    "\n",
    "The next step is to set up a RetrievalQA chain that combines the information retrieval process with the language generation model. The chain will search for the most relevant documents and pass them to Gemini for question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a RetrievalQA chain\n",
    "\n",
    "# 4. Fonction pour interroger le modèle Gemini avec les documents récupérés\n",
    "def query_to_gemini(query: str, top_k: int = 3):\n",
    "    # Générer l'embedding de la requête\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "    # Recherche des documents les plus proches dans l'index FAISS\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Récupérer les documents les plus proches\n",
    "    relevant_docs = [docs[i] for i in I[0]]\n",
    "\n",
    "    # Créer un prompt structuré avec les documents pertinents\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that answers questions based on the provided documents.\"),\n",
    "        (\"user\", f\"Here are some documents: {format_docs(relevant_docs)}\\n\\nQuestion: {query}\")\n",
    "    ])\n",
    "\n",
    "    # Formater le prompt pour l'API LangChain\n",
    "    formatted_prompt = prompt.format_messages(query=query)\n",
    "\n",
    "    # Interroger Gemini via LangChain pour obtenir une réponse\n",
    "    ai_msg = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # Vérifier si le message renvoyé par Gemini contient la réponse\n",
    "    if hasattr(ai_msg, 'content'):\n",
    "        return ai_msg.content\n",
    "    else:\n",
    "        return \"Erreur : la réponse générée ne contient pas de texte valide.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question.\n",
    "\n",
    "Once the system is set up, we can test it by asking questions. The system will retrieve relevant documents, generate the embeddings for the query, and pass them to Gemini for generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : The main topic appears to be RAG (Retrieval Augmented Generation) techniques, specifically focusing on different aspects like hierarchical indices, hypothetical question answering (HyDE), and choosing chunk size for document processing.  It also mentions tools and libraries related to document loading and parsing within the context of RAG.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# 5. Fonction pour formater les documents à afficher dans le prompt Gemini\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Answer:\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = query_to_gemini(query)\n",
    "print(f\"result : {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs.\n",
    "\n",
    "You can experiment with various questions to evaluate the system's performance and how well it retrieves relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : This document describes the self-attention mechanism, a key component in models like Transformers.  Here's a breakdown:\n",
      "\n",
      "* **Self-Attention Components:**  Self-attention involves three main elements:\n",
      "    * **Key (K):** Represents what information a word/token *has*.  Generated by multiplying the embedding (E) by a key matrix (WK).\n",
      "    * **Query (Q):** Represents what information a word/token is *looking for*. Generated by multiplying the embedding (E) by a query matrix (WQ).\n",
      "    * **Value (V):** Represents the information a word/token *reveals* to other words/tokens. Generated by multiplying the embedding (E) by a value matrix\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"Can you summarize the key points mentioned?\"\n",
    "result = query_to_gemini(query)\n",
    "print(f\"result : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- Sentence-Transformers: Generates embeddings that capture the semantic meaning of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
