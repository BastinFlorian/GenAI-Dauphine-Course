{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "filename = \"../data/gen_ai_course.txt\"\n",
    "# Answer:\n",
    "loader = TextLoader(filename, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# Answer\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embeddings and store them in a VectorStore\n",
    "# Hint: FAISS\n",
    "# Hint : Use GoogleGenerativeAIEmbeddings(model=...)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "# Initialize ChatGoogleGenerativeAI with the required arguments\n",
    "\n",
    "#Create a function to format documents for the prompt\n",
    "def format_docs(docs: List[Document]):\n",
    "    # Hint: Join the content of each document\n",
    "    return \"\\n\".join(doc.page_content for doc in docs) # Join the page content of docs into a stringg\n",
    "\n",
    "# Hint: Define the prompt template with system and human messages. See help below\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate answers based on these documents {documents}.\"),\n",
    "    (\"human\", \"Question: {question}\"),\n",
    "])\n",
    "\n",
    "# Hint: Format the documents using the function above\n",
    "formatted_docs = format_docs(docs) \n",
    "\n",
    "# Hint: Create the QA chain by combining the prompt and model\n",
    "qa_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provides a comprehensive overview of Generative AI with Large Language Models (LLMs). It covers various aspects, including the architecture of transformers, training processes for LLMs (pretraining and fine-tuning), Retrieval Augmented Generation (RAG), and the use of tools and agents with LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# Answer:\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document defines Language Models (LMs) as probability distributions over a sequence of words, denoted as p(x₁, …, xₙ).  They assign probabilities to sequences of words, reflecting syntactic and semantic knowledge.  Large Language Models (LLMs) are a subset of LMs, typically larger and more powerful, trained on massive datasets, and capable of generating human-quality text, translating languages, writing different kinds of creative content, and answering your questions in an informative way.  They function as generative models, predicting the next word in a sequence based on the preceding words (autoregressive).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What's a llm?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provides a comprehensive overview of Generative AI with Large Language Models (LLMs), covering the following key areas:\n",
      "\n",
      "* **Building LLMs:** This involves two phases: pre-training (training on a massive dataset to predict the next word in a sequence, using cross-entropy loss and techniques like byte-pair encoding for tokenization) and post-training (fine-tuning the model for specific tasks using supervised fine-tuning or reinforcement learning from human feedback (RLHF)).  Scaling laws dictate the optimal model and data size based on available compute resources.  Cost and optimization strategies are crucial for training large models.  RLHF, including reward models and algorithms like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), aligns the model with human preferences.  Evaluation is done using datasets like IFEval, BBH, MMLU-Pro, and others.\n",
      "\n",
      "* **Transformers:** The core architecture of LLMs, transformers rely on self-attention and cross-attention mechanisms to weigh the importance of different words in a sequence.  Multi-head attention allows the model to attend to different parts of the input simultaneously.  Residual connections and layer normalization help with training stability and performance.  Feed-forward layers and softmax functions are used for prediction.  Positional embeddings provide information about word order.\n",
      "\n",
      "* **Retrieval Augmented Generation (RAG):** This framework enhances LLMs by giving them access to external knowledge sources.  It involves information retrieval (using methods like TF-IDF, BM25, cosine similarity, and Maximal Marginal Relevance), vector databases for storing and querying embeddings, and techniques like query augmentation and re-ranking.  Evaluation metrics include precision, recall, and NDCG. Multimodal RAG extends this to other data types like images and videos. Advanced RAG architectures include Self-RAG, RAPTOR, and Corrective RAG.\n",
      "\n",
      "* **Tools and Agents:** LLMs can be augmented with tools to access and process external information, and agents can use these tools autonomously to perform complex tasks.  This is a rapidly evolving area of research.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"Can you summarize the key points ?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two main phases of building LLMs like GPT-3 are **pre-training** and **post-training**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the two main phases of building LLMs ?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention allows each word in a sequence to consider its relationship with every other word in the *same* sequence, helping the model understand the context and relationships within the input.  Cross-attention, used in encoder-decoder models like those for translation, allows each word in the decoder sequence to attend to every word in the *encoder* sequence, helping the model understand how the input and output sequences relate to each other.  In simpler terms, self-attention looks within a single sequence, while cross-attention looks at two different sequences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the role of self-attention and cross-attention in transforme?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents) \n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document covers a wide range of topics related to Large Language Models (LLMs), from their underlying architecture and training process to advanced techniques like Retrieval Augmented Generation (RAG) and the use of tools and agents.  Here's a summary of the key points:\n",
      "\n",
      "**Building LLMs:**\n",
      "\n",
      "* **Architecture:**  Modern LLMs are primarily based on the Transformer architecture, which utilizes self-attention mechanisms to process sequential data efficiently, overcoming limitations of previous recurrent models like RNNs and LSTMs.  Key components include multi-head attention, residual connections, layer normalization, feed-forward layers, a softmax layer, and positional embeddings.\n",
      "* **Training:**  Training involves a pre-training phase focused on language modeling (predicting the next word in a sequence) using a massive dataset, followed by a post-training phase to align the model with human preferences.  Techniques like Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) are crucial for this alignment.  Scaling laws dictate the relationship between model size, data size, and compute resources, influencing training decisions.  Tokenization is a critical pre-processing step.\n",
      "* **Evaluation:**  Evaluation is complex and involves metrics like perplexity and performance on specific datasets.  Evaluating alignment is challenging and often involves human judgment.\n",
      "\n",
      "**Retrieval Augmented Generation (RAG):**\n",
      "\n",
      "* **Concept:** RAG enhances LLMs by allowing them to access external knowledge sources during generation, improving accuracy and reducing hallucinations.  This involves retrieving relevant information from a knowledge base or vector store based on the user's query.\n",
      "* **Techniques:**  Various techniques are used in RAG, including different retrieval methods (TF-IDF, BM25, cosine similarity), vector databases, efficient similarity search algorithms (ScaNN, FAISS, HNSW), query augmentation, query rephrasing, and reranking of retrieved documents.  Multimodal RAG extends this to incorporate various data types like images and video.\n",
      "* **Architectures:** Advanced RAG architectures like Self-RAG, RAPTOR, Corrective RAG, and GraphRAG employ sophisticated techniques to improve retrieval and generation quality.\n",
      "* **Evaluation:** Evaluating RAG systems involves assessing both retriever performance (precision, recall, NDCG) and answer quality, often using LLMs as judges.\n",
      "\n",
      "**Tools and Agents:**\n",
      "\n",
      "* LLMs can be augmented with tools to access and process information from external sources, expanding their capabilities.\n",
      "* Agents utilize these tools to perform complex tasks by breaking them down into sub-tasks and interacting with the environment.\n",
      "\n",
      "The document also touches upon topics like prompt engineering, context size limitations and chunking strategies, and the rising importance of multimodal LLMs.  It emphasizes the ongoing evolution of LLM training and deployment, with new techniques constantly being developed to improve performance, alignment, and efficiency.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Can you summarize the key points ?\"\n",
    "result = qa_chain.run(documents=formatted_docs, question=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
