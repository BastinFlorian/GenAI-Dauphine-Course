{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System with LangChain\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will learn how to build a Retrieval-Augmented Generation (RAG) system using LangChain in Python. RAG systems combine information retrieval and natural language generation to produce answers that are grounded in external knowledge bases. This approach is particularly useful when dealing with large documents or datasets where direct querying isn’t efficient or possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the concept of Retrieval-Augmented Generation (RAG).\n",
    "- Learn how to use LangChain to implement a RAG system.\n",
    "- Implement the system step by step with guided TODO tasks.\n",
    "- Test your implementation at each step.\n",
    "- Provide helpful explanations and definitions.\n",
    "\n",
    "Help\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "- LangChain: A library for building language model applications.\n",
    "- VectorStore (FAISS): A tool for efficient similarity search and clustering of dense vectors.\n",
    "- OpenAI Embeddings: Representations of text that can capture semantic meaning.\n",
    "- RetrievalQA Chain: Combines retrieval and question-answering over documents.\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- I extracted some chapters of the Gen AI course as a txt file. \n",
    "- The goal how this notebook is to build a RAG system that can answer questions based on the content of these chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "\n",
    "We need to import the required modules and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI, hub\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Documents\n",
    "\n",
    "Load the document you want to use and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your document and split it into chunks\n",
    "# Hint: Use TextLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "filename = \"../data/gen_ai_course.txt\"\n",
    "# Answer:\n",
    "loader = TextLoader(filename)  \n",
    "documents = loader.load() \n",
    "\n",
    "# Answer:\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Build the VectorStore\n",
    "\n",
    "Generate embeddings for each chunk and store them in a vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embeddings and store them in a VectorStore\n",
    "# Hint: Use OpenAIEmbeddings and FAISS\n",
    "# Create embeddings and store them in a VectorStore\n",
    "# Answer:  \n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "#vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Initialise les embeddings avec la clé API chargée\n",
    "embeddings = GoogleGenerativeAIEmbeddings(api_key=os.getenv(\"GOOGLE_API_KEY\"), model=\"models/embedding-001\")\n",
    "\n",
    "# Créez le VectorStore avec FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the QA Chain using LCEL \n",
    "\n",
    "Create a chain that can retrieve relevant chunks and generate answers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a RetrievalQA chain\n",
    "# Hint: Use ChatOpenAI, create a prompt, and use StrOutputParser\n",
    "# Hint: The chain should be an LCEL chain https://python.langchain.com/v0.1/docs/expression_language/get_started/\n",
    "#llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "#prompt = ChatPromptTemplate.from_template(\n",
    "    #template=\"Based on the following documents, answer the user's question.\\n\\nDocuments:\\n{docs}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "#)\n",
    "\n",
    "#def format_docs(docs: List[Document]):\n",
    " #   return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Answer:\n",
    "#qa_chain = hub.rlm.rag_chain(\n",
    " #   retriever=vectorstore.as_retriever(),  \n",
    " #   llm=llm,  \n",
    " #   prompt=prompt,  \n",
    "  #  doc_formatter=format_docs, \n",
    "   # output_parser=StrOutputParser() \n",
    "#)  \n",
    "\n",
    "#Gemini\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    max_tokens=None,\n",
    "    timeout=20,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a question-answering chatbot. Provide answers based on the provided documents.\"),\n",
    "        (\"human\", \"Question: {question}\\n\\nRelevant Documents:\\n{formatted_docs}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_docs = format_docs(docs)\n",
    "\n",
    "qa_chain = prompt | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask Questions and Get Answers\n",
    "\n",
    "Test the system by asking a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document provides a comprehensive overview of Large Language Models (LLMs), focusing on their architecture, training, and applications, especially within the context of Generative AI.  It covers topics such as transformer models, pre-training and fine-tuning LLMs, retrieval augmented generation (RAG), and the use of tools and agents with LLMs.  It also touches upon the history of language models, leading up to the transformer architecture, and discusses various optimization techniques and evaluation methods.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question to the QA chain\n",
    "# Replace 'Your question here' with an actual question and run the qa_chain for this question\n",
    "\n",
    "# Answer:\n",
    "#query = \"What is the main topic discussed in the document?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "\n",
    "query_1 = \"What is the main topic discussed in the document?\"\n",
    "result = qa_chain.invoke({\n",
    "    \"question\": query_1,\n",
    "    \"formatted_docs\": formatted_docs\n",
    "}).content\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Your Implementation with Different Questions\n",
    "\n",
    "Try out different questions to see how the system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='This document covers a wide range of topics related to Large Language Models (LLMs), focusing on their construction, training, optimization, and applications.\\n\\n**Key Concepts:**\\n\\n* **Building LLMs:** This involves pre-training (using cross-entropy loss, tokenization (BPE), data preprocessing, scaling laws) and post-training (fine-tuning).  Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF) using reward models and Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO) are crucial for aligning LLMs with human preferences. Evaluation is done using datasets like IFEval, BBH, MMLU-Pro, and Math.\\n* **Transformers Architecture:** The core of LLMs, replacing older architectures like RNNs and LSTMs. Key components include self-attention/cross-attention, multi-head attention, residual connections, layer normalization, feed-forward layers, softmax layer, and positional embeddings.  Attention mechanisms allow the model to weigh the importance of different parts of the input sequence.\\n* **Retrieval Augmented Generation (RAG):**  Enhances LLMs by connecting them to external knowledge sources. It involves information retrieval (TF-IDF, BM25, cosine similarity), vector databases, search optimization (ScaNN, FAISS, HNSW), and techniques like query augmentation, rephrasing, and chunking.  Evaluation uses precision, recall, NDCG, and LLM-based judges. Multimodal RAG extends this to other data types like images. Advanced RAG architectures include Self-RAG, RAPTOR, Corrective RAG, and GraphRAG.\\n* **Tools and Agents:**  LLMs can be augmented with tools to access and process external information, and agents can use these tools to perform complex tasks.\\n* **Optimization Techniques:**  Includes scaling laws (Chinchilla), cost optimization, and methods like LoRA and quantization.\\n\\nThe document emphasizes the importance of data preprocessing, the evolution from RNNs/LSTMs to Transformers, the power of RAG for improving LLM accuracy and reducing hallucinations, and the potential of tools and agents for expanding LLM capabilities.  It also highlights the ongoing challenges in evaluation and the continuous development of new techniques like DPO and advanced RAG architectures.\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-3b4ab62b-8baf-4e36-8196-c38784a35b53-0' usage_metadata={'input_tokens': 27085, 'output_tokens': 465, 'total_tokens': 27550, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Another question here' with your own question and run the qa_chain for this question\n",
    "\n",
    "query = \"Can you summarize the key points mentioned?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "result = qa_chain.invoke({  \n",
    "    \"question\": query,      \n",
    "    \"formatted_docs\": formatted_docs  \n",
    "})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"The Transformer architecture replaces recurrent networks with a self-attention mechanism.  Key components include:\\n\\n* **Self-Attention/Cross Attention:** This mechanism allows the model to weigh the importance of different parts of the input sequence when generating an output.  Self-attention focuses on relationships within a single sequence, while cross-attention relates two different sequences.\\n* **Multi-Head Attention:**  This performs multiple attention calculations in parallel, allowing the model to capture different relationships between words.  These parallel calculations are then combined.\\n* **Residual Connections & Layer Normalization:** Residual connections help mitigate vanishing gradients and keep information local. Layer normalization stabilizes hidden state dynamics.\\n* **Feed Forward Layer:**  This consists of two linear transformations with a ReLU activation in between, applied to each position independently.  It helps the model learn complex non-linear relationships.\\n* **Softmax Layer:** This converts the model's output into a probability distribution over the vocabulary, allowing for next-word prediction.\\n* **Positional Embeddings:** Since the Transformer doesn't process sequentially, positional embeddings are added to the input to provide information about word order.  These are often sinusoidal functions.\\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-b43485ba-308f-47c1-852c-f15ae090393b-0' usage_metadata={'input_tokens': 27086, 'output_tokens': 242, 'total_tokens': 27328, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"Can you summarize the Transformer's Architecture?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "result = qa_chain.invoke({  \n",
    "    \"question\": query_2,      \n",
    "    \"formatted_docs\": formatted_docs  \n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The main difference between a Transformer and an LSTM lies in how they process sequential data. LSTMs process data sequentially, maintaining a \"memory\" of past information using a cell state and gates to control information flow.  This sequential nature makes them slower for long sequences and susceptible to vanishing gradients, although LSTMs mitigate this issue better than standard RNNs.  Transformers, on the other hand, process data in parallel using the attention mechanism. This allows them to capture relationships between all words in a sequence simultaneously, leading to better performance on long sequences and faster training times.  The attention mechanism also allows Transformers to weigh the importance of different parts of the input sequence, making them more effective at capturing long-range dependencies.  While LSTMs were a significant improvement over standard RNNs, Transformers have largely superseded them for many NLP tasks due to their superior performance and parallelization capabilities.\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-3c62f8d7-56ef-40c6-b355-f4346acd57ad-0' usage_metadata={'input_tokens': 27089, 'output_tokens': 176, 'total_tokens': 27265, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"What is the main difference between a Transformer and an LSTM ?\"\n",
    "#result = qa_chain.invoke({\"question\": query})\n",
    "#print(result)\n",
    "\n",
    "result = qa_chain.invoke({  \n",
    "    \"question\": query_3,      \n",
    "    \"formatted_docs\": formatted_docs  \n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Improve the System\n",
    "\n",
    "You can experiment with different parameters, like adjusting the chunk size or using a different language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Congratulations! You’ve built a simple Retrieval-Augmented Generation system using LangChain. This system can retrieve relevant information from documents and generate answers to user queries.\n",
    "\n",
    "Help\n",
    "\n",
    "- TextLoader: Loads text data from files.\n",
    "- RecursiveCharacterTextSplitter: Splits text into smaller chunks for better processing.\n",
    "- FAISS: A library for efficient similarity search of embeddings.\n",
    "- RetrievalQA Chain: A chain that retrieves relevant documents and answers questions based on them.\n",
    "- OpenAIEmbeddings: Generates embeddings that capture the semantic meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# ChatPromptValue(\n",
    "#    messages=[\n",
    "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
    "#        HumanMessage(content='Hello, how are you doing?'),\n",
    "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "#        HumanMessage(content='What is your name?')\n",
    "#    ]\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
